{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQnuX3C-kaK3",
    "outputId": "1bde8e24-08c8-422c-df83-50ed1e3e52df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.5.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "# Install transformers package from Huggingface\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SsAw5LhAkdLE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9GgWRFm5rMs"
   },
   "source": [
    "## Detect GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TtIwr5MmkdNe",
    "outputId": "5445790f-dbc5-4986-84ee-21cb3a36cffa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU  Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "  print('Using GPU ', torch.cuda.get_device_name(0)) \n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "  print('Using CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2WUFvD_5xM2"
   },
   "source": [
    "## Load data and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HAA8dqB2kdQA",
    "outputId": "158536df-0768-4a6a-b915-70c80c66ecaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length for 10K data: 944\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('10K_text_price_label.csv')\n",
    "\n",
    "print(\"The length for 10K data: {}\".format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7pNz7tLpakN",
    "outputId": "d30a92f2-6bbd-42c8-d9cb-2ea2ecd08d26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "fzkIWPpRkdSn",
    "outputId": "7143eb19-540d-4acd-ac18-13bd542d9ef0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Year</th>\n",
       "      <th>Doc</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACC</td>\n",
       "      <td>2017</td>\n",
       "      <td>10 k 1 acc2016123110k htm 10 k document unite ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACC</td>\n",
       "      <td>2016</td>\n",
       "      <td>10 k 1 acc2015123110k htm 10 k 10 k unite stat...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACC</td>\n",
       "      <td>2015</td>\n",
       "      <td>10 k 1 acc2014123110k htm 10 k acc 2014 12 31 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACC</td>\n",
       "      <td>2014</td>\n",
       "      <td>10 k 1 acc2013123110k htm 10 k acc 2013 12 31 ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACC</td>\n",
       "      <td>2013</td>\n",
       "      <td>10 k 1 t75648_10k htm form 10 k t75648_10k htm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Company  Year                                                Doc  Label\n",
       "0     ACC  2017  10 k 1 acc2016123110k htm 10 k document unite ...      0\n",
       "1     ACC  2016  10 k 1 acc2015123110k htm 10 k 10 k unite stat...      2\n",
       "2     ACC  2015  10 k 1 acc2014123110k htm 10 k acc 2014 12 31 ...      1\n",
       "3     ACC  2014  10 k 1 acc2013123110k htm 10 k acc 2013 12 31 ...      2\n",
       "4     ACC  2013  10 k 1 t75648_10k htm form 10 k t75648_10k htm...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_qV2MQu58Mk"
   },
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pmM4Jv0ykdVC"
   },
   "outputs": [],
   "source": [
    "# Split the data set\n",
    "doc_data = df[['Doc']].to_numpy()\n",
    "doc_data = doc_data.reshape(doc_data.shape[0])\n",
    "labels = df[['Label']].to_numpy()\n",
    "labels = labels.reshape(labels.shape[0])\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(doc_data, labels, test_size=0.2, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9l-w6Jmhdk3U",
    "outputId": "fafdeedd-bae3-4c3d-c8dc-7609120d6cf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 0 0 2 2 2 2 0 2 2 1 2 0 0 0 0 2 2 2 2 1 0 0 2 2 2 2 2 2 2 2 2 2 0 0 2\n",
      " 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iA8vaSiE6CyZ"
   },
   "source": [
    "## Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "aY0S0wrVkdXT"
   },
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "vocab = 'voc_uniq.txt'\n",
    "tokenizer = BertTokenizer(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXyAVdeE6Jqw"
   },
   "source": [
    "## Train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0-JqhvJokdZ1"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "BATCHES = 8\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4GvO0EjJkdcW"
   },
   "outputs": [],
   "source": [
    "# Turn labels and encodings into a Dataset object\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "YtkoDExXkdew",
    "outputId": "b99b673e-a15d-4335-9aff-78492c37cb30"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9465e23c0fab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Set the optimizer AdamW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Implement eary stopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Encoding the training data\n",
    "train_encoding = tokenizer(list(train_texts), return_tensors='pt', padding=True, truncation=True, max_length=30)\n",
    "\n",
    "# Encoding the testing data\n",
    "test_encoding = tokenizer(list(test_texts), return_tensors='pt', padding=True, truncation=True, max_length=30)\n",
    "test_dataset = MyDataset(test_encoding, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCHES, shuffle=False)\n",
    "\n",
    "\n",
    "# Turn into dataset object\n",
    "train_dataset = MyDataset(train_encoding, train_labels)\n",
    "\n",
    "# Use mini-bathces\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCHES, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_pjnwmrOpyWZ",
    "outputId": "b7daa5a3-dae9-4bc9-8ee0-f4022429664a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Epoch: 1\n",
      "------------------------------------------\n",
      "Train loss: 102.11060512065887\n",
      "\n",
      "Current Epoch: 2\n",
      "------------------------------------------\n",
      "Train loss: 99.58523297309875\n",
      "\n",
      "Current Epoch: 3\n",
      "------------------------------------------\n",
      "Train loss: 100.18051385879517\n",
      "\n",
      "Current Epoch: 4\n",
      "------------------------------------------\n",
      "Train loss: 99.78292000293732\n",
      "\n",
      "Current Epoch: 5\n",
      "------------------------------------------\n",
      "Train loss: 100.15877145528793\n",
      "\n",
      "Current Epoch: 6\n",
      "------------------------------------------\n",
      "Train loss: 100.38584458827972\n",
      "\n",
      "Current Epoch: 7\n",
      "------------------------------------------\n",
      "Train loss: 100.1389040350914\n",
      "\n",
      "Stop training because of the early stop at epoch 7\n"
     ]
    }
   ],
   "source": [
    "# Set current learning rate here\n",
    "best_lr = 1e-5\n",
    "\n",
    "# Bert model from Huggingface\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', return_dict=True, num_labels=3)\n",
    "\n",
    "# Set the optimizer AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=best_lr)\n",
    "\n",
    "# Implement early stopping\n",
    "min_loss = float('inf')\n",
    "epoch_count = 0\n",
    "early_stop = False\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# Put the model on device\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Put the model in training mode\n",
    "  model.train()\n",
    "\n",
    "  train_loss = 0\n",
    "\n",
    "  for batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    batch_labels = batch['labels'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "    # Use cross entropy loss\n",
    "    #loss = F.cross_entropy(outputs.logits, batch_labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "  \n",
    "  print(\"Current Epoch: {}\".format(epoch + 1))\n",
    "  print(\"------------------------------------------\")\n",
    "  print(\"Train loss: {}\".format(train_loss))\n",
    "  print()\n",
    "  \n",
    "  # Check whether to stop or not\n",
    "  min_loss = min(train_loss, min_loss)\n",
    "  if min_loss < train_loss:\n",
    "    if epoch_count == 4:\n",
    "      early_stop = True\n",
    "      print(\"Stop training because of the early stop at epoch {}\".format(epoch + 1))\n",
    "      break\n",
    "    else:\n",
    "      epoch_count += 1\n",
    "  else:\n",
    "    # Reset the count\n",
    "    epoch_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKx1_bhE6OAf"
   },
   "source": [
    "## Report the evaluation result on testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4BJclDzgkdg8",
    "outputId": "e7d12504-5beb-4487-9cb6-cae4e3c5408b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precison: 0.48677248677248675\n",
      "Recall: 0.48677248677248675\n",
      "F1 score: 0.48677248677248675\n",
      "Accuracy: 0.48677248677248675\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    y_pred = None\n",
    "    for batch in test_loader:\n",
    "        \n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      batch_labels = batch['labels'].to(device)\n",
    "      output = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "\n",
    "      _, predicted_labels = torch.max(output.logits, 1)\n",
    "      if y_pred is not None:\n",
    "          y_pred = torch.cat((y_pred, predicted_labels), 0)\n",
    "      else:\n",
    "          y_pred = predicted_labels\n",
    " \n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, y_pred.cpu(), average='micro')\n",
    "acc = accuracy_score(test_labels, y_pred.cpu())\n",
    "print('Precison: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1 score: {}'.format(f1))\n",
    "print('Accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93sPZNzDVLFw",
    "outputId": "74d04240-1612-4854-d19d-2e66ea32b99d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Epoch: 1\n",
      "------------------------------------------\n",
      "Train loss: 102.34586662054062\n",
      "\n",
      "Current Epoch: 2\n",
      "------------------------------------------\n",
      "Train loss: 102.1022360920906\n",
      "\n",
      "Current Epoch: 3\n",
      "------------------------------------------\n",
      "Train loss: 101.73578315973282\n",
      "\n",
      "Current Epoch: 4\n",
      "------------------------------------------\n",
      "Train loss: 100.83886557817459\n",
      "\n",
      "Current Epoch: 5\n",
      "------------------------------------------\n",
      "Train loss: 101.27074784040451\n",
      "\n",
      "Current Epoch: 6\n",
      "------------------------------------------\n",
      "Train loss: 100.65839612483978\n",
      "\n",
      "Current Epoch: 7\n",
      "------------------------------------------\n",
      "Train loss: 101.88309514522552\n",
      "\n",
      "Current Epoch: 8\n",
      "------------------------------------------\n",
      "Train loss: 100.74715292453766\n",
      "\n",
      "Current Epoch: 9\n",
      "------------------------------------------\n",
      "Train loss: 100.01596808433533\n",
      "\n",
      "Current Epoch: 10\n",
      "------------------------------------------\n",
      "Train loss: 101.4622488617897\n",
      "\n",
      "Current Epoch: 11\n",
      "------------------------------------------\n",
      "Train loss: 100.87228679656982\n",
      "\n",
      "Current Epoch: 12\n",
      "------------------------------------------\n",
      "Train loss: 101.09428364038467\n",
      "\n",
      "Current Epoch: 13\n",
      "------------------------------------------\n",
      "Train loss: 100.59808003902435\n",
      "\n",
      "Current Epoch: 14\n",
      "------------------------------------------\n",
      "Train loss: 101.44308578968048\n",
      "\n",
      "Stop training because of the early stop at epoch 14\n"
     ]
    }
   ],
   "source": [
    "# Set current learning rate here\n",
    "best_lr = 1e-4\n",
    "\n",
    "# Bert model from Huggingface\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', return_dict=True, num_labels=3)\n",
    "\n",
    "# Set the optimizer AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=best_lr)\n",
    "\n",
    "# Implement early stopping\n",
    "min_loss = float('inf')\n",
    "epoch_count = 0\n",
    "early_stop = False\n",
    "\n",
    "# Put the model on device\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Put the model in training mode\n",
    "  model.train()\n",
    "\n",
    "  train_loss = 0\n",
    "\n",
    "  for batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    batch_labels = batch['labels'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "    # Use cross entropy loss\n",
    "    #loss = F.cross_entropy(outputs.logits, batch_labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "  \n",
    "  print(\"Current Epoch: {}\".format(epoch + 1))\n",
    "  print(\"------------------------------------------\")\n",
    "  print(\"Train loss: {}\".format(train_loss))\n",
    "  print()\n",
    "  \n",
    "  # Check whether to stop or not\n",
    "  min_loss = min(train_loss, min_loss)\n",
    "  if min_loss < train_loss:\n",
    "    if epoch_count == 4:\n",
    "      early_stop = True\n",
    "      print(\"Stop training because of the early stop at epoch {}\".format(epoch + 1))\n",
    "      break\n",
    "    else:\n",
    "      epoch_count += 1\n",
    "  else:\n",
    "    # Reset the count\n",
    "    epoch_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xzsSThthVLJu",
    "outputId": "99ea0300-52be-41b4-f509-f2e5274e988a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precison: 0.48677248677248675\n",
      "Recall: 0.48677248677248675\n",
      "F1 score: 0.48677248677248675\n",
      "Accuracy: 0.48677248677248675\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    y_pred = None\n",
    "    for batch in test_loader:\n",
    "        \n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      batch_labels = batch['labels'].to(device)\n",
    "      output = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "\n",
    "      _, predicted_labels = torch.max(output.logits, 1)\n",
    "      if y_pred is not None:\n",
    "          y_pred = torch.cat((y_pred, predicted_labels), 0)\n",
    "      else:\n",
    "          y_pred = predicted_labels\n",
    " \n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, y_pred.cpu(), average='micro')\n",
    "acc = accuracy_score(test_labels, y_pred.cpu())\n",
    "print('Precison: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1 score: {}'.format(f1))\n",
    "print('Accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YDYdvTiJVfUP",
    "outputId": "2a3a7132-37d2-4c27-ad5d-1e42953d4282"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Epoch: 1\n",
      "------------------------------------------\n",
      "Train loss: 99.9067815542221\n",
      "\n",
      "Current Epoch: 2\n",
      "------------------------------------------\n",
      "Train loss: 100.09157240390778\n",
      "\n",
      "Current Epoch: 3\n",
      "------------------------------------------\n",
      "Train loss: 100.16233003139496\n",
      "\n",
      "Current Epoch: 4\n",
      "------------------------------------------\n",
      "Train loss: 99.6149154305458\n",
      "\n",
      "Current Epoch: 5\n",
      "------------------------------------------\n",
      "Train loss: 99.32338970899582\n",
      "\n",
      "Current Epoch: 6\n",
      "------------------------------------------\n",
      "Train loss: 100.39861404895782\n",
      "\n",
      "Current Epoch: 7\n",
      "------------------------------------------\n",
      "Train loss: 99.96470230817795\n",
      "\n",
      "Current Epoch: 8\n",
      "------------------------------------------\n",
      "Train loss: 100.0778598189354\n",
      "\n",
      "Current Epoch: 9\n",
      "------------------------------------------\n",
      "Train loss: 99.69874411821365\n",
      "\n",
      "Current Epoch: 10\n",
      "------------------------------------------\n",
      "Train loss: 99.81671100854874\n",
      "\n",
      "Stop training because of the early stop at epoch 10\n"
     ]
    }
   ],
   "source": [
    "# Set current learning rate here\n",
    "best_lr = 1e-6\n",
    "\n",
    "# Bert model from Huggingface\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', return_dict=True, num_labels=3)\n",
    "\n",
    "# Set the optimizer AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=best_lr)\n",
    "\n",
    "# Implement early stopping\n",
    "min_loss = float('inf')\n",
    "epoch_count = 0\n",
    "early_stop = False\n",
    "\n",
    "# Put the model on device\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Put the model in training mode\n",
    "  model.train()\n",
    "\n",
    "  train_loss = 0\n",
    "\n",
    "  for batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    batch_labels = batch['labels'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "    # Use cross entropy loss\n",
    "    #loss = F.cross_entropy(outputs.logits, batch_labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "  \n",
    "  print(\"Current Epoch: {}\".format(epoch + 1))\n",
    "  print(\"------------------------------------------\")\n",
    "  print(\"Train loss: {}\".format(train_loss))\n",
    "  print()\n",
    "  \n",
    "  # Check whether to stop or not\n",
    "  min_loss = min(train_loss, min_loss)\n",
    "  if min_loss < train_loss:\n",
    "    if epoch_count == 4:\n",
    "      early_stop = True\n",
    "      print(\"Stop training because of the early stop at epoch {}\".format(epoch + 1))\n",
    "      break\n",
    "    else:\n",
    "      epoch_count += 1\n",
    "  else:\n",
    "    # Reset the count\n",
    "    epoch_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38V_IQZKVfMZ",
    "outputId": "ddbd42ed-fd26-4af7-c269-e8a4fbade1c3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precison: 0.48677248677248675\n",
      "Recall: 0.48677248677248675\n",
      "F1 score: 0.48677248677248675\n",
      "Accuracy: 0.48677248677248675\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    y_pred = None\n",
    "    for batch in test_loader:\n",
    "        \n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      batch_labels = batch['labels'].to(device)\n",
    "      output = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "\n",
    "      _, predicted_labels = torch.max(output.logits, 1)\n",
    "      if y_pred is not None:\n",
    "          y_pred = torch.cat((y_pred, predicted_labels), 0)\n",
    "      else:\n",
    "          y_pred = predicted_labels\n",
    " \n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, y_pred.cpu(), average='micro')\n",
    "acc = accuracy_score(test_labels, y_pred.cpu())\n",
    "print('Precison: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1 score: {}'.format(f1))\n",
    "print('Accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sscibq6CVn3r",
    "outputId": "d2c9dcf2-42f3-49ce-c658-f8fd8f767cb3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Epoch: 1\n",
      "------------------------------------------\n",
      "Train loss: 112.64965546131134\n",
      "\n",
      "Current Epoch: 2\n",
      "------------------------------------------\n",
      "Train loss: 105.52300530672073\n",
      "\n",
      "Current Epoch: 3\n",
      "------------------------------------------\n",
      "Train loss: 113.17386874556541\n",
      "\n",
      "Current Epoch: 4\n",
      "------------------------------------------\n",
      "Train loss: 106.17731446027756\n",
      "\n",
      "Current Epoch: 5\n",
      "------------------------------------------\n",
      "Train loss: 107.82152622938156\n",
      "\n",
      "Current Epoch: 6\n",
      "------------------------------------------\n",
      "Train loss: 107.28663957118988\n",
      "\n",
      "Current Epoch: 7\n",
      "------------------------------------------\n",
      "Train loss: 108.04945343732834\n",
      "\n",
      "Stop training because of the early stop at epoch 7\n"
     ]
    }
   ],
   "source": [
    "# Set current learning rate here\n",
    "best_lr = 1e-3\n",
    "\n",
    "# Bert model from Huggingface\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', return_dict=True, num_labels=3)\n",
    "\n",
    "# Set the optimizer AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=best_lr)\n",
    "\n",
    "# Implement early stopping\n",
    "min_loss = float('inf')\n",
    "epoch_count = 0\n",
    "early_stop = False\n",
    "\n",
    "# Put the model on device\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Put the model in training mode\n",
    "  model.train()\n",
    "\n",
    "  train_loss = 0\n",
    "\n",
    "  for batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    batch_labels = batch['labels'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "    # Use cross entropy loss\n",
    "    #loss = F.cross_entropy(outputs.logits, batch_labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "  \n",
    "  print(\"Current Epoch: {}\".format(epoch + 1))\n",
    "  print(\"------------------------------------------\")\n",
    "  print(\"Train loss: {}\".format(train_loss))\n",
    "  print()\n",
    "  \n",
    "  # Check whether to stop or not\n",
    "  min_loss = min(train_loss, min_loss)\n",
    "  if min_loss < train_loss:\n",
    "    if epoch_count == 4:\n",
    "      early_stop = True\n",
    "      print(\"Stop training because of the early stop at epoch {}\".format(epoch + 1))\n",
    "      break\n",
    "    else:\n",
    "      epoch_count += 1\n",
    "  else:\n",
    "    # Reset the count\n",
    "    epoch_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uNOUjXldVn-I",
    "outputId": "3cd42f6d-af24-483f-b10e-75b039ae9665"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precison: 0.43386243386243384\n",
      "Recall: 0.43386243386243384\n",
      "F1 score: 0.43386243386243384\n",
      "Accuracy: 0.43386243386243384\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    y_pred = None\n",
    "    for batch in test_loader:\n",
    "        \n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      batch_labels = batch['labels'].to(device)\n",
    "      output = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "\n",
    "      _, predicted_labels = torch.max(output.logits, 1)\n",
    "      if y_pred is not None:\n",
    "          y_pred = torch.cat((y_pred, predicted_labels), 0)\n",
    "      else:\n",
    "          y_pred = predicted_labels\n",
    " \n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, y_pred.cpu(), average='micro')\n",
    "acc = accuracy_score(test_labels, y_pred.cpu())\n",
    "print('Precison: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1 score: {}'.format(f1))\n",
    "print('Accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EQoeyJCeUBs"
   },
   "source": [
    "### Encoding length 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "XqHTBYhaeJ2X"
   },
   "outputs": [],
   "source": [
    "# Encoding the training data\n",
    "train_encoding = tokenizer(list(train_texts), return_tensors='pt', padding=True, truncation=True, max_length=50)\n",
    "\n",
    "# Encoding the testing data\n",
    "test_encoding = tokenizer(list(test_texts), return_tensors='pt', padding=True, truncation=True, max_length=50)\n",
    "test_dataset = MyDataset(test_encoding, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCHES, shuffle=False)\n",
    "\n",
    "\n",
    "# Turn into dataset object\n",
    "train_dataset = MyDataset(train_encoding, train_labels)\n",
    "\n",
    "# Use mini-bathces\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCHES, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XZ2kgzQ7eJ8w",
    "outputId": "82c47509-9abb-4d52-e5a8-4abfefd0f529"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Epoch: 1\n",
      "------------------------------------------\n",
      "Train loss: 102.08098077774048\n",
      "\n",
      "Current Epoch: 2\n",
      "------------------------------------------\n",
      "Train loss: 101.13652354478836\n",
      "\n",
      "Current Epoch: 3\n",
      "------------------------------------------\n",
      "Train loss: 100.77958822250366\n",
      "\n",
      "Current Epoch: 4\n",
      "------------------------------------------\n",
      "Train loss: 100.42237436771393\n",
      "\n",
      "Current Epoch: 5\n",
      "------------------------------------------\n",
      "Train loss: 99.8835666179657\n",
      "\n",
      "Current Epoch: 6\n",
      "------------------------------------------\n",
      "Train loss: 100.40240788459778\n",
      "\n",
      "Current Epoch: 7\n",
      "------------------------------------------\n",
      "Train loss: 100.12757760286331\n",
      "\n",
      "Current Epoch: 8\n",
      "------------------------------------------\n",
      "Train loss: 99.86036270856857\n",
      "\n",
      "Current Epoch: 9\n",
      "------------------------------------------\n",
      "Train loss: 100.02865767478943\n",
      "\n",
      "Current Epoch: 10\n",
      "------------------------------------------\n",
      "Train loss: 99.84298491477966\n",
      "\n",
      "Current Epoch: 11\n",
      "------------------------------------------\n",
      "Train loss: 99.97733825445175\n",
      "\n",
      "Current Epoch: 12\n",
      "------------------------------------------\n",
      "Train loss: 99.76372456550598\n",
      "\n",
      "Current Epoch: 13\n",
      "------------------------------------------\n",
      "Train loss: 99.88206869363785\n",
      "\n",
      "Current Epoch: 14\n",
      "------------------------------------------\n",
      "Train loss: 99.75278127193451\n",
      "\n",
      "Current Epoch: 15\n",
      "------------------------------------------\n",
      "Train loss: 99.9510247707367\n",
      "\n",
      "Current Epoch: 16\n",
      "------------------------------------------\n",
      "Train loss: 99.54631441831589\n",
      "\n",
      "Current Epoch: 17\n",
      "------------------------------------------\n",
      "Train loss: 99.39214515686035\n",
      "\n",
      "Current Epoch: 18\n",
      "------------------------------------------\n",
      "Train loss: 99.84958589076996\n",
      "\n",
      "Current Epoch: 19\n",
      "------------------------------------------\n",
      "Train loss: 99.75126612186432\n",
      "\n",
      "Current Epoch: 20\n",
      "------------------------------------------\n",
      "Train loss: 99.32166212797165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set current learning rate here\n",
    "best_lr = 1e-4\n",
    "\n",
    "# Bert model from Huggingface\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', return_dict=True, num_labels=3)\n",
    "\n",
    "# Set the optimizer AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=best_lr)\n",
    "\n",
    "# Implement early stopping\n",
    "min_loss = float('inf')\n",
    "epoch_count = 0\n",
    "early_stop = False\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# Put the model on device\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Put the model in training mode\n",
    "  model.train()\n",
    "\n",
    "  train_loss = 0\n",
    "\n",
    "  for batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    batch_labels = batch['labels'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "    # Use cross entropy loss\n",
    "    #loss = F.cross_entropy(outputs.logits, batch_labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "  \n",
    "  print(\"Current Epoch: {}\".format(epoch + 1))\n",
    "  print(\"------------------------------------------\")\n",
    "  print(\"Train loss: {}\".format(train_loss))\n",
    "  print()\n",
    "  \n",
    "  # Check whether to stop or not\n",
    "  min_loss = min(train_loss, min_loss)\n",
    "  if min_loss < train_loss:\n",
    "    if epoch_count == 4:\n",
    "      early_stop = True\n",
    "      print(\"Stop training because of the early stop at epoch {}\".format(epoch + 1))\n",
    "      break\n",
    "    else:\n",
    "      epoch_count += 1\n",
    "  else:\n",
    "    # Reset the count\n",
    "    epoch_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hlIxrigveKG3",
    "outputId": "75d684e1-15c5-4729-d81e-f672fd735c73"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precison: 0.48677248677248675\n",
      "Recall: 0.48677248677248675\n",
      "F1 score: 0.48677248677248675\n",
      "Accuracy: 0.48677248677248675\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    y_pred = None\n",
    "    for batch in test_loader:\n",
    "        \n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      batch_labels = batch['labels'].to(device)\n",
    "      output = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "\n",
    "      _, predicted_labels = torch.max(output.logits, 1)\n",
    "      if y_pred is not None:\n",
    "          y_pred = torch.cat((y_pred, predicted_labels), 0)\n",
    "      else:\n",
    "          y_pred = predicted_labels\n",
    " \n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, y_pred.cpu(), average='micro')\n",
    "acc = accuracy_score(test_labels, y_pred.cpu())\n",
    "print('Precison: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1 score: {}'.format(f1))\n",
    "print('Accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DxcsbWAseKQy",
    "outputId": "23e7cbfc-e471-4cca-8aa8-ff614095f119"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Epoch: 1\n",
      "------------------------------------------\n",
      "Train loss: 100.94346356391907\n",
      "\n",
      "Current Epoch: 2\n",
      "------------------------------------------\n",
      "Train loss: 100.76950865983963\n",
      "\n",
      "Current Epoch: 3\n",
      "------------------------------------------\n",
      "Train loss: 100.24304419755936\n",
      "\n",
      "Current Epoch: 4\n",
      "------------------------------------------\n",
      "Train loss: 100.50902831554413\n",
      "\n",
      "Current Epoch: 5\n",
      "------------------------------------------\n",
      "Train loss: 99.9561225771904\n",
      "\n",
      "Current Epoch: 6\n",
      "------------------------------------------\n",
      "Train loss: 99.95437431335449\n",
      "\n",
      "Current Epoch: 7\n",
      "------------------------------------------\n",
      "Train loss: 99.61865335702896\n",
      "\n",
      "Current Epoch: 8\n",
      "------------------------------------------\n",
      "Train loss: 99.83063042163849\n",
      "\n",
      "Current Epoch: 9\n",
      "------------------------------------------\n",
      "Train loss: 99.94023770093918\n",
      "\n",
      "Current Epoch: 10\n",
      "------------------------------------------\n",
      "Train loss: 99.15044224262238\n",
      "\n",
      "Current Epoch: 11\n",
      "------------------------------------------\n",
      "Train loss: 99.61254405975342\n",
      "\n",
      "Current Epoch: 12\n",
      "------------------------------------------\n",
      "Train loss: 99.71735644340515\n",
      "\n",
      "Current Epoch: 13\n",
      "------------------------------------------\n",
      "Train loss: 99.26117271184921\n",
      "\n",
      "Current Epoch: 14\n",
      "------------------------------------------\n",
      "Train loss: 99.2613719701767\n",
      "\n",
      "Current Epoch: 15\n",
      "------------------------------------------\n",
      "Train loss: 100.43856990337372\n",
      "\n",
      "Stop training because of the early stop at epoch 15\n"
     ]
    }
   ],
   "source": [
    "# Set current learning rate here\n",
    "best_lr = 1e-5\n",
    "\n",
    "# Bert model from Huggingface\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', return_dict=True, num_labels=3)\n",
    "\n",
    "# Set the optimizer AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=best_lr)\n",
    "\n",
    "# Implement early stopping\n",
    "min_loss = float('inf')\n",
    "epoch_count = 0\n",
    "early_stop = False\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# Put the model on device\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Put the model in training mode\n",
    "  model.train()\n",
    "\n",
    "  train_loss = 0\n",
    "\n",
    "  for batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    batch_labels = batch['labels'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "    # Use cross entropy loss\n",
    "    #loss = F.cross_entropy(outputs.logits, batch_labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "  \n",
    "  print(\"Current Epoch: {}\".format(epoch + 1))\n",
    "  print(\"------------------------------------------\")\n",
    "  print(\"Train loss: {}\".format(train_loss))\n",
    "  print()\n",
    "  \n",
    "  # Check whether to stop or not\n",
    "  min_loss = min(train_loss, min_loss)\n",
    "  if min_loss < train_loss:\n",
    "    if epoch_count == 4:\n",
    "      early_stop = True\n",
    "      print(\"Stop training because of the early stop at epoch {}\".format(epoch + 1))\n",
    "      break\n",
    "    else:\n",
    "      epoch_count += 1\n",
    "  else:\n",
    "    # Reset the count\n",
    "    epoch_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KReHdwa3eKMT",
    "outputId": "78bfc3e7-10c5-4ed1-8c76-99ff033e678a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precison: 0.48677248677248675\n",
      "Recall: 0.48677248677248675\n",
      "F1 score: 0.48677248677248675\n",
      "Accuracy: 0.48677248677248675\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    y_pred = None\n",
    "    for batch in test_loader:\n",
    "        \n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      batch_labels = batch['labels'].to(device)\n",
    "      output = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "\n",
    "      _, predicted_labels = torch.max(output.logits, 1)\n",
    "      if y_pred is not None:\n",
    "          y_pred = torch.cat((y_pred, predicted_labels), 0)\n",
    "      else:\n",
    "          y_pred = predicted_labels\n",
    " \n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, y_pred.cpu(), average='micro')\n",
    "acc = accuracy_score(test_labels, y_pred.cpu())\n",
    "print('Precison: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1 score: {}'.format(f1))\n",
    "print('Accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u42C0vYXemaA",
    "outputId": "0e89f7aa-d534-4023-c32f-0b531c77aca4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Epoch: 1\n",
      "------------------------------------------\n",
      "Train loss: 101.31260031461716\n",
      "\n",
      "Current Epoch: 2\n",
      "------------------------------------------\n",
      "Train loss: 99.91304397583008\n",
      "\n",
      "Current Epoch: 3\n",
      "------------------------------------------\n",
      "Train loss: 99.96644991636276\n",
      "\n",
      "Current Epoch: 4\n",
      "------------------------------------------\n",
      "Train loss: 100.3437032699585\n",
      "\n",
      "Current Epoch: 5\n",
      "------------------------------------------\n",
      "Train loss: 98.9865118265152\n",
      "\n",
      "Current Epoch: 6\n",
      "------------------------------------------\n",
      "Train loss: 99.35534924268723\n",
      "\n",
      "Current Epoch: 7\n",
      "------------------------------------------\n",
      "Train loss: 99.77695482969284\n",
      "\n",
      "Current Epoch: 8\n",
      "------------------------------------------\n",
      "Train loss: 99.81931245326996\n",
      "\n",
      "Current Epoch: 9\n",
      "------------------------------------------\n",
      "Train loss: 99.29109394550323\n",
      "\n",
      "Current Epoch: 10\n",
      "------------------------------------------\n",
      "Train loss: 99.29612857103348\n",
      "\n",
      "Stop training because of the early stop at epoch 10\n"
     ]
    }
   ],
   "source": [
    "# Set current learning rate here\n",
    "best_lr = 1e-6\n",
    "\n",
    "# Bert model from Huggingface\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', return_dict=True, num_labels=3)\n",
    "\n",
    "# Set the optimizer AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=best_lr)\n",
    "\n",
    "# Implement early stopping\n",
    "min_loss = float('inf')\n",
    "epoch_count = 0\n",
    "early_stop = False\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# Put the model on device\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Put the model in training mode\n",
    "  model.train()\n",
    "\n",
    "  train_loss = 0\n",
    "\n",
    "  for batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    batch_labels = batch['labels'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "    # Use cross entropy loss\n",
    "    #loss = F.cross_entropy(outputs.logits, batch_labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "  \n",
    "  print(\"Current Epoch: {}\".format(epoch + 1))\n",
    "  print(\"------------------------------------------\")\n",
    "  print(\"Train loss: {}\".format(train_loss))\n",
    "  print()\n",
    "  \n",
    "  # Check whether to stop or not\n",
    "  min_loss = min(train_loss, min_loss)\n",
    "  if min_loss < train_loss:\n",
    "    if epoch_count == 4:\n",
    "      early_stop = True\n",
    "      print(\"Stop training because of the early stop at epoch {}\".format(epoch + 1))\n",
    "      break\n",
    "    else:\n",
    "      epoch_count += 1\n",
    "  else:\n",
    "    # Reset the count\n",
    "    epoch_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rtwoJI3YemS6",
    "outputId": "61d9b740-80b6-4284-f632-a43d5c01ca1d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precison: 0.48677248677248675\n",
      "Recall: 0.48677248677248675\n",
      "F1 score: 0.48677248677248675\n",
      "Accuracy: 0.48677248677248675\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    y_pred = None\n",
    "    for batch in test_loader:\n",
    "        \n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      batch_labels = batch['labels'].to(device)\n",
    "      output = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "\n",
    "      _, predicted_labels = torch.max(output.logits, 1)\n",
    "      if y_pred is not None:\n",
    "          y_pred = torch.cat((y_pred, predicted_labels), 0)\n",
    "      else:\n",
    "          y_pred = predicted_labels\n",
    " \n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, y_pred.cpu(), average='micro')\n",
    "acc = accuracy_score(test_labels, y_pred.cpu())\n",
    "print('Precison: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1 score: {}'.format(f1))\n",
    "print('Accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTDXDsragWpQ"
   },
   "source": [
    "## Encoding length 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "onN8JPylgrhm"
   },
   "outputs": [],
   "source": [
    "# Encoding the training data\n",
    "train_encoding = tokenizer(list(train_texts), return_tensors='pt', padding=True, truncation=True, max_length=60)\n",
    "\n",
    "# Encoding the testing data\n",
    "test_encoding = tokenizer(list(test_texts), return_tensors='pt', padding=True, truncation=True, max_length=60)\n",
    "test_dataset = MyDataset(test_encoding, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCHES, shuffle=False)\n",
    "\n",
    "\n",
    "# Turn into dataset object\n",
    "train_dataset = MyDataset(train_encoding, train_labels)\n",
    "\n",
    "# Use mini-bathces\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCHES, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-srimzDSemPA",
    "outputId": "d521f321-59d1-4556-9674-0ca64df7c589"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Epoch: 1\n",
      "------------------------------------------\n",
      "Train loss: 100.5548887848854\n",
      "\n",
      "Current Epoch: 2\n",
      "------------------------------------------\n",
      "Train loss: 101.72064644098282\n",
      "\n",
      "Current Epoch: 3\n",
      "------------------------------------------\n",
      "Train loss: 100.79745942354202\n",
      "\n",
      "Current Epoch: 4\n",
      "------------------------------------------\n",
      "Train loss: 100.60223823785782\n",
      "\n",
      "Current Epoch: 5\n",
      "------------------------------------------\n",
      "Train loss: 100.14266192913055\n",
      "\n",
      "Current Epoch: 6\n",
      "------------------------------------------\n",
      "Train loss: 101.54773724079132\n",
      "\n",
      "Current Epoch: 7\n",
      "------------------------------------------\n",
      "Train loss: 101.1067144870758\n",
      "\n",
      "Current Epoch: 8\n",
      "------------------------------------------\n",
      "Train loss: 100.57700783014297\n",
      "\n",
      "Current Epoch: 9\n",
      "------------------------------------------\n",
      "Train loss: 100.46186876296997\n",
      "\n",
      "Current Epoch: 10\n",
      "------------------------------------------\n",
      "Train loss: 100.69149053096771\n",
      "\n",
      "Stop training because of the early stop at epoch 10\n",
      "Precison: 0.48677248677248675\n",
      "Recall: 0.48677248677248675\n",
      "F1 score: 0.48677248677248675\n",
      "Accuracy: 0.48677248677248675\n"
     ]
    }
   ],
   "source": [
    "# Set current learning rate here\n",
    "best_lr = 1e-4\n",
    "\n",
    "# Bert model from Huggingface\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', return_dict=True, num_labels=3)\n",
    "\n",
    "# Set the optimizer AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=best_lr)\n",
    "\n",
    "# Implement early stopping\n",
    "min_loss = float('inf')\n",
    "epoch_count = 0\n",
    "early_stop = False\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# Put the model on device\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Put the model in training mode\n",
    "  model.train()\n",
    "\n",
    "  train_loss = 0\n",
    "\n",
    "  for batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    batch_labels = batch['labels'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "    # Use cross entropy loss\n",
    "    #loss = F.cross_entropy(outputs.logits, batch_labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "  \n",
    "  print(\"Current Epoch: {}\".format(epoch + 1))\n",
    "  print(\"------------------------------------------\")\n",
    "  print(\"Train loss: {}\".format(train_loss))\n",
    "  print()\n",
    "  \n",
    "  # Check whether to stop or not\n",
    "  min_loss = min(train_loss, min_loss)\n",
    "  if min_loss < train_loss:\n",
    "    if epoch_count == 4:\n",
    "      early_stop = True\n",
    "      print(\"Stop training because of the early stop at epoch {}\".format(epoch + 1))\n",
    "      break\n",
    "    else:\n",
    "      epoch_count += 1\n",
    "  else:\n",
    "    # Reset the count\n",
    "    epoch_count = 0\n",
    "  \n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    y_pred = None\n",
    "    for batch in test_loader:\n",
    "        \n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      batch_labels = batch['labels'].to(device)\n",
    "      output = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "\n",
    "      _, predicted_labels = torch.max(output.logits, 1)\n",
    "      if y_pred is not None:\n",
    "          y_pred = torch.cat((y_pred, predicted_labels), 0)\n",
    "      else:\n",
    "          y_pred = predicted_labels\n",
    " \n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, y_pred.cpu(), average='micro')\n",
    "acc = accuracy_score(test_labels, y_pred.cpu())\n",
    "print('Precison: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1 score: {}'.format(f1))\n",
    "print('Accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uADaA0xRemFN",
    "outputId": "6142ca61-0de9-46a0-bcf9-bb050a853cec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Epoch: 1\n",
      "------------------------------------------\n",
      "Train loss: 100.22936367988586\n",
      "\n",
      "Current Epoch: 2\n",
      "------------------------------------------\n",
      "Train loss: 100.50483506917953\n",
      "\n",
      "Current Epoch: 3\n",
      "------------------------------------------\n",
      "Train loss: 99.00542706251144\n",
      "\n",
      "Current Epoch: 4\n",
      "------------------------------------------\n",
      "Train loss: 99.68203139305115\n",
      "\n",
      "Current Epoch: 5\n",
      "------------------------------------------\n",
      "Train loss: 100.07219403982162\n",
      "\n",
      "Current Epoch: 6\n",
      "------------------------------------------\n",
      "Train loss: 99.46432965993881\n",
      "\n",
      "Current Epoch: 7\n",
      "------------------------------------------\n",
      "Train loss: 99.74774277210236\n",
      "\n",
      "Current Epoch: 8\n",
      "------------------------------------------\n",
      "Train loss: 100.82446074485779\n",
      "\n",
      "Stop training because of the early stop at epoch 8\n",
      "Precison: 0.48677248677248675\n",
      "Recall: 0.48677248677248675\n",
      "F1 score: 0.48677248677248675\n",
      "Accuracy: 0.48677248677248675\n"
     ]
    }
   ],
   "source": [
    "# Set current learning rate here\n",
    "best_lr = 1e-5\n",
    "\n",
    "# Bert model from Huggingface\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', return_dict=True, num_labels=3)\n",
    "\n",
    "# Set the optimizer AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=best_lr)\n",
    "\n",
    "# Implement early stopping\n",
    "min_loss = float('inf')\n",
    "epoch_count = 0\n",
    "early_stop = False\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# Put the model on device\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Put the model in training mode\n",
    "  model.train()\n",
    "\n",
    "  train_loss = 0\n",
    "\n",
    "  for batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    batch_labels = batch['labels'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "    # Use cross entropy loss\n",
    "    #loss = F.cross_entropy(outputs.logits, batch_labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "  \n",
    "  print(\"Current Epoch: {}\".format(epoch + 1))\n",
    "  print(\"------------------------------------------\")\n",
    "  print(\"Train loss: {}\".format(train_loss))\n",
    "  print()\n",
    "  \n",
    "  # Check whether to stop or not\n",
    "  min_loss = min(train_loss, min_loss)\n",
    "  if min_loss < train_loss:\n",
    "    if epoch_count == 4:\n",
    "      early_stop = True\n",
    "      print(\"Stop training because of the early stop at epoch {}\".format(epoch + 1))\n",
    "      break\n",
    "    else:\n",
    "      epoch_count += 1\n",
    "  else:\n",
    "    # Reset the count\n",
    "    epoch_count = 0\n",
    "  \n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    y_pred = None\n",
    "    for batch in test_loader:\n",
    "        \n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      batch_labels = batch['labels'].to(device)\n",
    "      output = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "\n",
    "      _, predicted_labels = torch.max(output.logits, 1)\n",
    "      if y_pred is not None:\n",
    "          y_pred = torch.cat((y_pred, predicted_labels), 0)\n",
    "      else:\n",
    "          y_pred = predicted_labels\n",
    " \n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, y_pred.cpu(), average='micro')\n",
    "acc = accuracy_score(test_labels, y_pred.cpu())\n",
    "print('Precison: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1 score: {}'.format(f1))\n",
    "print('Accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RtqtM2t4emBj",
    "outputId": "e1956c6e-de0a-4431-b1bf-92cb095f4381"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Epoch: 1\n",
      "------------------------------------------\n",
      "Train loss: 100.42273378372192\n",
      "\n",
      "Current Epoch: 2\n",
      "------------------------------------------\n",
      "Train loss: 99.82439410686493\n",
      "\n",
      "Current Epoch: 3\n",
      "------------------------------------------\n",
      "Train loss: 99.67061275243759\n",
      "\n",
      "Current Epoch: 4\n",
      "------------------------------------------\n",
      "Train loss: 99.76746356487274\n",
      "\n",
      "Current Epoch: 5\n",
      "------------------------------------------\n",
      "Train loss: 99.71128237247467\n",
      "\n",
      "Current Epoch: 6\n",
      "------------------------------------------\n",
      "Train loss: 99.23172569274902\n",
      "\n",
      "Current Epoch: 7\n",
      "------------------------------------------\n",
      "Train loss: 99.06269037723541\n",
      "\n",
      "Current Epoch: 8\n",
      "------------------------------------------\n",
      "Train loss: 99.05891364812851\n",
      "\n",
      "Current Epoch: 9\n",
      "------------------------------------------\n",
      "Train loss: 99.33373576402664\n",
      "\n",
      "Current Epoch: 10\n",
      "------------------------------------------\n",
      "Train loss: 99.37162804603577\n",
      "\n",
      "Current Epoch: 11\n",
      "------------------------------------------\n",
      "Train loss: 99.31048965454102\n",
      "\n",
      "Current Epoch: 12\n",
      "------------------------------------------\n",
      "Train loss: 99.59732782840729\n",
      "\n",
      "Current Epoch: 13\n",
      "------------------------------------------\n",
      "Train loss: 99.04494571685791\n",
      "\n",
      "Current Epoch: 14\n",
      "------------------------------------------\n",
      "Train loss: 100.17729669809341\n",
      "\n",
      "Current Epoch: 15\n",
      "------------------------------------------\n",
      "Train loss: 99.93444389104843\n",
      "\n",
      "Current Epoch: 16\n",
      "------------------------------------------\n",
      "Train loss: 99.31670224666595\n",
      "\n",
      "Current Epoch: 17\n",
      "------------------------------------------\n",
      "Train loss: 99.48968601226807\n",
      "\n",
      "Current Epoch: 18\n",
      "------------------------------------------\n",
      "Train loss: 99.72673237323761\n",
      "\n",
      "Stop training because of the early stop at epoch 18\n",
      "Precison: 0.48677248677248675\n",
      "Recall: 0.48677248677248675\n",
      "F1 score: 0.48677248677248675\n",
      "Accuracy: 0.48677248677248675\n"
     ]
    }
   ],
   "source": [
    "# Set current learning rate here\n",
    "best_lr = 1e-6\n",
    "\n",
    "# Bert model from Huggingface\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', return_dict=True, num_labels=3)\n",
    "\n",
    "# Set the optimizer AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=best_lr)\n",
    "\n",
    "# Implement early stopping\n",
    "min_loss = float('inf')\n",
    "epoch_count = 0\n",
    "early_stop = False\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# Put the model on device\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Put the model in training mode\n",
    "  model.train()\n",
    "\n",
    "  train_loss = 0\n",
    "\n",
    "  for batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    batch_labels = batch['labels'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "    # Use cross entropy loss\n",
    "    #loss = F.cross_entropy(outputs.logits, batch_labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "  \n",
    "  print(\"Current Epoch: {}\".format(epoch + 1))\n",
    "  print(\"------------------------------------------\")\n",
    "  print(\"Train loss: {}\".format(train_loss))\n",
    "  print()\n",
    "  \n",
    "  # Check whether to stop or not\n",
    "  min_loss = min(train_loss, min_loss)\n",
    "  if min_loss < train_loss:\n",
    "    if epoch_count == 4:\n",
    "      early_stop = True\n",
    "      print(\"Stop training because of the early stop at epoch {}\".format(epoch + 1))\n",
    "      break\n",
    "    else:\n",
    "      epoch_count += 1\n",
    "  else:\n",
    "    # Reset the count\n",
    "    epoch_count = 0\n",
    "  \n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    y_pred = None\n",
    "    for batch in test_loader:\n",
    "        \n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      batch_labels = batch['labels'].to(device)\n",
    "      output = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "\n",
    "      _, predicted_labels = torch.max(output.logits, 1)\n",
    "      if y_pred is not None:\n",
    "          y_pred = torch.cat((y_pred, predicted_labels), 0)\n",
    "      else:\n",
    "          y_pred = predicted_labels\n",
    " \n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, y_pred.cpu(), average='micro')\n",
    "acc = accuracy_score(test_labels, y_pred.cpu())\n",
    "print('Precison: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1 score: {}'.format(f1))\n",
    "print('Accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rp4wAQGpRNT0"
   },
   "source": [
    "### Encoding length 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nThesUS9el7C"
   },
   "outputs": [],
   "source": [
    "# Encoding the training data\n",
    "train_encoding = tokenizer(list(train_texts), return_tensors='pt', padding=True, truncation=True, max_length=20)\n",
    "\n",
    "# Encoding the testing data\n",
    "test_encoding = tokenizer(list(test_texts), return_tensors='pt', padding=True, truncation=True, max_length=20)\n",
    "test_dataset = MyDataset(test_encoding, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCHES, shuffle=False)\n",
    "\n",
    "\n",
    "# Turn into dataset object\n",
    "train_dataset = MyDataset(train_encoding, train_labels)\n",
    "\n",
    "# Use mini-bathces\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCHES, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zXaeUBtel0v"
   },
   "outputs": [],
   "source": [
    "# Set current learning rate here\n",
    "best_lr = 1e-4\n",
    "\n",
    "# Bert model from Huggingface\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', return_dict=True, num_labels=3)\n",
    "\n",
    "# Set the optimizer AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=best_lr)\n",
    "\n",
    "# Implement early stopping\n",
    "min_loss = float('inf')\n",
    "epoch_count = 0\n",
    "early_stop = False\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# Put the model on device\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Put the model in training mode\n",
    "  model.train()\n",
    "\n",
    "  train_loss = 0\n",
    "\n",
    "  for batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    batch_labels = batch['labels'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "    # Use cross entropy loss\n",
    "    #loss = F.cross_entropy(outputs.logits, batch_labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "  \n",
    "  print(\"Current Epoch: {}\".format(epoch + 1))\n",
    "  print(\"------------------------------------------\")\n",
    "  print(\"Train loss: {}\".format(train_loss))\n",
    "  print()\n",
    "  \n",
    "  # Check whether to stop or not\n",
    "  min_loss = min(train_loss, min_loss)\n",
    "  if min_loss < train_loss:\n",
    "    if epoch_count == 4:\n",
    "      early_stop = True\n",
    "      print(\"Stop training because of the early stop at epoch {}\".format(epoch + 1))\n",
    "      break\n",
    "    else:\n",
    "      epoch_count += 1\n",
    "  else:\n",
    "    # Reset the count\n",
    "    epoch_count = 0\n",
    "  \n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    y_pred = None\n",
    "    for batch in test_loader:\n",
    "        \n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      batch_labels = batch['labels'].to(device)\n",
    "      output = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "\n",
    "      _, predicted_labels = torch.max(output.logits, 1)\n",
    "      if y_pred is not None:\n",
    "          y_pred = torch.cat((y_pred, predicted_labels), 0)\n",
    "      else:\n",
    "          y_pred = predicted_labels\n",
    " \n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, y_pred.cpu(), average='micro')\n",
    "acc = accuracy_score(test_labels, y_pred.cpu())\n",
    "print('Precison: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1 score: {}'.format(f1))\n",
    "print('Accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K_igH-J6RnSF"
   },
   "outputs": [],
   "source": [
    "# Set current learning rate here\n",
    "best_lr = 1e-5\n",
    "\n",
    "# Bert model from Huggingface\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', return_dict=True, num_labels=3)\n",
    "\n",
    "# Set the optimizer AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=best_lr)\n",
    "\n",
    "# Implement early stopping\n",
    "min_loss = float('inf')\n",
    "epoch_count = 0\n",
    "early_stop = False\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# Put the model on device\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Put the model in training mode\n",
    "  model.train()\n",
    "\n",
    "  train_loss = 0\n",
    "\n",
    "  for batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    batch_labels = batch['labels'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "    # Use cross entropy loss\n",
    "    #loss = F.cross_entropy(outputs.logits, batch_labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "  \n",
    "  print(\"Current Epoch: {}\".format(epoch + 1))\n",
    "  print(\"------------------------------------------\")\n",
    "  print(\"Train loss: {}\".format(train_loss))\n",
    "  print()\n",
    "  \n",
    "  # Check whether to stop or not\n",
    "  min_loss = min(train_loss, min_loss)\n",
    "  if min_loss < train_loss:\n",
    "    if epoch_count == 4:\n",
    "      early_stop = True\n",
    "      print(\"Stop training because of the early stop at epoch {}\".format(epoch + 1))\n",
    "      break\n",
    "    else:\n",
    "      epoch_count += 1\n",
    "  else:\n",
    "    # Reset the count\n",
    "    epoch_count = 0\n",
    "  \n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    y_pred = None\n",
    "    for batch in test_loader:\n",
    "        \n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      batch_labels = batch['labels'].to(device)\n",
    "      output = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "\n",
    "      _, predicted_labels = torch.max(output.logits, 1)\n",
    "      if y_pred is not None:\n",
    "          y_pred = torch.cat((y_pred, predicted_labels), 0)\n",
    "      else:\n",
    "          y_pred = predicted_labels\n",
    " \n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, y_pred.cpu(), average='micro')\n",
    "acc = accuracy_score(test_labels, y_pred.cpu())\n",
    "print('Precison: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1 score: {}'.format(f1))\n",
    "print('Accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fV76NVDNelwB"
   },
   "outputs": [],
   "source": [
    "# Set current learning rate here\n",
    "best_lr = 1e-6\n",
    "\n",
    "# Bert model from Huggingface\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', return_dict=True, num_labels=3)\n",
    "\n",
    "# Set the optimizer AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=best_lr)\n",
    "\n",
    "# Implement early stopping\n",
    "min_loss = float('inf')\n",
    "epoch_count = 0\n",
    "early_stop = False\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# Put the model on device\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Put the model in training mode\n",
    "  model.train()\n",
    "\n",
    "  train_loss = 0\n",
    "\n",
    "  for batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    batch_labels = batch['labels'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "    # Use cross entropy loss\n",
    "    #loss = F.cross_entropy(outputs.logits, batch_labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "  \n",
    "  print(\"Current Epoch: {}\".format(epoch + 1))\n",
    "  print(\"------------------------------------------\")\n",
    "  print(\"Train loss: {}\".format(train_loss))\n",
    "  print()\n",
    "  \n",
    "  # Check whether to stop or not\n",
    "  min_loss = min(train_loss, min_loss)\n",
    "  if min_loss < train_loss:\n",
    "    if epoch_count == 4:\n",
    "      early_stop = True\n",
    "      print(\"Stop training because of the early stop at epoch {}\".format(epoch + 1))\n",
    "      break\n",
    "    else:\n",
    "      epoch_count += 1\n",
    "  else:\n",
    "    # Reset the count\n",
    "    epoch_count = 0\n",
    "  \n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    y_pred = None\n",
    "    for batch in test_loader:\n",
    "        \n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      batch_labels = batch['labels'].to(device)\n",
    "      output = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "\n",
    "      _, predicted_labels = torch.max(output.logits, 1)\n",
    "      if y_pred is not None:\n",
    "          y_pred = torch.cat((y_pred, predicted_labels), 0)\n",
    "      else:\n",
    "          y_pred = predicted_labels\n",
    " \n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, y_pred.cpu(), average='micro')\n",
    "acc = accuracy_score(test_labels, y_pred.cpu())\n",
    "print('Precison: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1 score: {}'.format(f1))\n",
    "print('Accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaVX-JyGRQWi"
   },
   "source": [
    "## Encoding length 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EU7u98UFRX7t"
   },
   "outputs": [],
   "source": [
    "# Encoding the training data\n",
    "train_encoding = tokenizer(list(train_texts), return_tensors='pt', padding=True, truncation=True, max_length=30)\n",
    "\n",
    "# Encoding the testing data\n",
    "test_encoding = tokenizer(list(test_texts), return_tensors='pt', padding=True, truncation=True, max_length=30)\n",
    "test_dataset = MyDataset(test_encoding, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCHES, shuffle=False)\n",
    "\n",
    "\n",
    "# Turn into dataset object\n",
    "train_dataset = MyDataset(train_encoding, train_labels)\n",
    "\n",
    "# Use mini-bathces\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCHES, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVG6cCs8RYGI"
   },
   "outputs": [],
   "source": [
    "# Set current learning rate here\n",
    "best_lr = 1e-4\n",
    "\n",
    "# Bert model from Huggingface\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', return_dict=True, num_labels=3)\n",
    "\n",
    "# Set the optimizer AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=best_lr)\n",
    "\n",
    "# Implement early stopping\n",
    "min_loss = float('inf')\n",
    "epoch_count = 0\n",
    "early_stop = False\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# Put the model on device\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Put the model in training mode\n",
    "  model.train()\n",
    "\n",
    "  train_loss = 0\n",
    "\n",
    "  for batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    batch_labels = batch['labels'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "    # Use cross entropy loss\n",
    "    #loss = F.cross_entropy(outputs.logits, batch_labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "  \n",
    "  print(\"Current Epoch: {}\".format(epoch + 1))\n",
    "  print(\"------------------------------------------\")\n",
    "  print(\"Train loss: {}\".format(train_loss))\n",
    "  print()\n",
    "  \n",
    "  # Check whether to stop or not\n",
    "  min_loss = min(train_loss, min_loss)\n",
    "  if min_loss < train_loss:\n",
    "    if epoch_count == 4:\n",
    "      early_stop = True\n",
    "      print(\"Stop training because of the early stop at epoch {}\".format(epoch + 1))\n",
    "      break\n",
    "    else:\n",
    "      epoch_count += 1\n",
    "  else:\n",
    "    # Reset the count\n",
    "    epoch_count = 0\n",
    "  \n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    y_pred = None\n",
    "    for batch in test_loader:\n",
    "        \n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      batch_labels = batch['labels'].to(device)\n",
    "      output = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "\n",
    "      _, predicted_labels = torch.max(output.logits, 1)\n",
    "      if y_pred is not None:\n",
    "          y_pred = torch.cat((y_pred, predicted_labels), 0)\n",
    "      else:\n",
    "          y_pred = predicted_labels\n",
    " \n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, y_pred.cpu(), average='micro')\n",
    "acc = accuracy_score(test_labels, y_pred.cpu())\n",
    "print('Precison: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1 score: {}'.format(f1))\n",
    "print('Accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oWjc8G-VRssH"
   },
   "outputs": [],
   "source": [
    "# Set current learning rate here\n",
    "best_lr = 1e-5\n",
    "\n",
    "# Bert model from Huggingface\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', return_dict=True, num_labels=3)\n",
    "\n",
    "# Set the optimizer AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=best_lr)\n",
    "\n",
    "# Implement early stopping\n",
    "min_loss = float('inf')\n",
    "epoch_count = 0\n",
    "early_stop = False\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# Put the model on device\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Put the model in training mode\n",
    "  model.train()\n",
    "\n",
    "  train_loss = 0\n",
    "\n",
    "  for batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    batch_labels = batch['labels'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "    # Use cross entropy loss\n",
    "    #loss = F.cross_entropy(outputs.logits, batch_labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "  \n",
    "  print(\"Current Epoch: {}\".format(epoch + 1))\n",
    "  print(\"------------------------------------------\")\n",
    "  print(\"Train loss: {}\".format(train_loss))\n",
    "  print()\n",
    "  \n",
    "  # Check whether to stop or not\n",
    "  min_loss = min(train_loss, min_loss)\n",
    "  if min_loss < train_loss:\n",
    "    if epoch_count == 4:\n",
    "      early_stop = True\n",
    "      print(\"Stop training because of the early stop at epoch {}\".format(epoch + 1))\n",
    "      break\n",
    "    else:\n",
    "      epoch_count += 1\n",
    "  else:\n",
    "    # Reset the count\n",
    "    epoch_count = 0\n",
    "  \n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    y_pred = None\n",
    "    for batch in test_loader:\n",
    "        \n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      batch_labels = batch['labels'].to(device)\n",
    "      output = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "\n",
    "      _, predicted_labels = torch.max(output.logits, 1)\n",
    "      if y_pred is not None:\n",
    "          y_pred = torch.cat((y_pred, predicted_labels), 0)\n",
    "      else:\n",
    "          y_pred = predicted_labels\n",
    " \n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, y_pred.cpu(), average='micro')\n",
    "acc = accuracy_score(test_labels, y_pred.cpu())\n",
    "print('Precison: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1 score: {}'.format(f1))\n",
    "print('Accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IxICmENvRYRS"
   },
   "outputs": [],
   "source": [
    "# Set current learning rate here\n",
    "best_lr = 1e-6\n",
    "\n",
    "# Bert model from Huggingface\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', return_dict=True, num_labels=3)\n",
    "\n",
    "# Set the optimizer AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=best_lr)\n",
    "\n",
    "# Implement early stopping\n",
    "min_loss = float('inf')\n",
    "epoch_count = 0\n",
    "early_stop = False\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# Put the model on device\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Put the model in training mode\n",
    "  model.train()\n",
    "\n",
    "  train_loss = 0\n",
    "\n",
    "  for batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    batch_labels = batch['labels'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "    # Use cross entropy loss\n",
    "    #loss = F.cross_entropy(outputs.logits, batch_labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "  \n",
    "  print(\"Current Epoch: {}\".format(epoch + 1))\n",
    "  print(\"------------------------------------------\")\n",
    "  print(\"Train loss: {}\".format(train_loss))\n",
    "  print()\n",
    "  \n",
    "  # Check whether to stop or not\n",
    "  min_loss = min(train_loss, min_loss)\n",
    "  if min_loss < train_loss:\n",
    "    if epoch_count == 4:\n",
    "      early_stop = True\n",
    "      print(\"Stop training because of the early stop at epoch {}\".format(epoch + 1))\n",
    "      break\n",
    "    else:\n",
    "      epoch_count += 1\n",
    "  else:\n",
    "    # Reset the count\n",
    "    epoch_count = 0\n",
    "  \n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    y_pred = None\n",
    "    for batch in test_loader:\n",
    "        \n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      batch_labels = batch['labels'].to(device)\n",
    "      output = model(input_ids, attention_mask=attention_mask, labels=batch_labels)\n",
    "\n",
    "      _, predicted_labels = torch.max(output.logits, 1)\n",
    "      if y_pred is not None:\n",
    "          y_pred = torch.cat((y_pred, predicted_labels), 0)\n",
    "      else:\n",
    "          y_pred = predicted_labels\n",
    " \n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, y_pred.cpu(), average='micro')\n",
    "acc = accuracy_score(test_labels, y_pred.cpu())\n",
    "print('Precison: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1 score: {}'.format(f1))\n",
    "print('Accuracy: {}'.format(acc))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Bert-on-10k.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
