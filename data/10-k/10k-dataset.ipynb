{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10K Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pprint\n",
    "import project_helper\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download NLP Corpora\n",
    "Use the stopwords and wordnet from nltk to preprocess the dataset later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/kathy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/kathy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get 10Ks Dataset\n",
    "We will use the company's unique CIK (Central Index Key) to identify each company.  \n",
    "Find the CIK through: https://www.sec.gov/edgar/searchedgar/companysearch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the company CIK here\n",
    "cik_lookup = {\n",
    "    'AMZN': '0001018724',\n",
    "    'FB': '0001326801',   \n",
    "    'GOOG': '0001652044',\n",
    "    'MSFT': '0000789019',\n",
    "    'TSLA': '0001318605'}\n",
    "\n",
    "additional_cik = {\n",
    "    'AEP': '0000004904',\n",
    "    'AXP': '0000004962',\n",
    "    'BA': '0000012927', \n",
    "    'BK': '0001390777',\n",
    "    'CAT': '0000018230',\n",
    "    'DE': '0000315189',\n",
    "    'DIS': '0001001039',\n",
    "    'DTE': '0000936340',\n",
    "    'ED': '0001047862',\n",
    "    'EMR': '0000032604',\n",
    "    'ETN': '0001551182',\n",
    "    'GE': '0000040545',\n",
    "    'IBM': '0000051143',\n",
    "    'IP': '0000051434',\n",
    "    'JNJ': '0000200406',\n",
    "    'KO': '0000021344',\n",
    "    'LLY': '0000059478',\n",
    "    'MCD': '0000063908',\n",
    "    'MO': '0000764180',\n",
    "    'MRK': '0000310158',\n",
    "    'MRO': '0000101778',\n",
    "    'PCG': '0001004980',\n",
    "    'PEP': '0000077476',\n",
    "    'PFE': '0000078003',\n",
    "    'PG': '0000080424',\n",
    "    'PNR': '0000077360',\n",
    "    'SYY': '0000096021',\n",
    "    'TXN': '0000097476',\n",
    "    'UTX': '0000101829',\n",
    "    'WFC': '0000072971',\n",
    "    'WMT': '0000104169',\n",
    "    'WY': '0000106535',\n",
    "    'XOM': '0000034088'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SecAPI to get the 10Ks data\n",
    "sec_api = project_helper.SecAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull a lost of filled 10-ks from the API for each company\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_sec_data(cik, doc_type, start=0, count=60):\n",
    "    rss_url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany' \\\n",
    "        '&CIK={}&type={}&start={}&count={}&owner=exclude&output=atom' \\\n",
    "        .format(cik, doc_type, start, count)\n",
    "    sec_data = sec_api.get(rss_url)\n",
    "    feed = BeautifulSoup(sec_data.encode('ascii'), 'xml').feed\n",
    "    entries = [\n",
    "        (\n",
    "            entry.content.find('filing-href').getText(),\n",
    "            entry.content.find('filing-type').getText(),\n",
    "            entry.content.find('filing-date').getText())\n",
    "        for entry in feed.find_all('entry', recursive=False)]\n",
    "\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https://www.sec.gov/Archives/edgar/data/1018724/000101872420000004/0001018724-20-000004-index.htm',\n",
      "  '10-K',\n",
      "  '2020-01-31'),\n",
      " ('https://www.sec.gov/Archives/edgar/data/1018724/000101872419000004/0001018724-19-000004-index.htm',\n",
      "  '10-K',\n",
      "  '2019-02-01'),\n",
      " ('https://www.sec.gov/Archives/edgar/data/1018724/000101872418000005/0001018724-18-000005-index.htm',\n",
      "  '10-K',\n",
      "  '2018-02-02'),\n",
      " ('https://www.sec.gov/Archives/edgar/data/1018724/000101872417000011/0001018724-17-000011-index.htm',\n",
      "  '10-K',\n",
      "  '2017-02-10'),\n",
      " ('https://www.sec.gov/Archives/edgar/data/1018724/000101872416000172/0001018724-16-000172-index.htm',\n",
      "  '10-K',\n",
      "  '2016-01-29')]\n"
     ]
    }
   ],
   "source": [
    "# Pull the data, and show one of the examples\n",
    "example_ticker = 'AMZN'\n",
    "sec_data = {}\n",
    "\n",
    "for ticker, cik in cik_lookup.items():\n",
    "    sec_data[ticker] = get_sec_data(cik, '10-K')\n",
    "\n",
    "pprint.pprint(sec_data[example_ticker][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading AMZN Fillings: 100%|██████████| 25/25 [00:07<00:00,  3.14filling/s]\n",
      "Downloading FB Fillings: 100%|██████████| 10/10 [00:05<00:00,  1.73filling/s]\n",
      "Downloading GOOG Fillings: 100%|██████████| 7/7 [00:02<00:00,  3.03filling/s]\n",
      "Downloading MSFT Fillings: 100%|██████████| 27/27 [00:09<00:00,  2.81filling/s]\n",
      "Downloading TSLA Fillings: 100%|██████████| 12/12 [00:05<00:00,  2.07filling/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Document:\n",
      "\n",
      "<SEC-DOCUMENT>0001018724-20-000004.txt : 20200131\n",
      "<SEC-HEADER>0001018724-20-000004.hdr.sgml : 20200131\n",
      "<ACCEPTANCE-DATETIME>20200130204613\n",
      "ACCESSION NUMBER:\t\t0001018724-20-000004\n",
      "CONFORMED SUBMISSION TYPE:\t10-K\n",
      "PUBLIC DOCUMENT COUNT:\t\t109\n",
      "CONFORMED PERIOD OF REPORT:\t20191231\n",
      "FILED AS OF DATE:\t\t20200131\n",
      "DATE AS OF CHANGE:\t\t20200130\n",
      "\n",
      "FILER:\n",
      "\n",
      "\tCOMPANY DATA:\t\n",
      "\t\tCOMPANY CONFORMED NAME:\t\t\tAMAZON COM INC\n",
      "\t\tCENTRAL INDEX KEY:\t\t\t0001018724\n",
      "\t\tSTANDARD INDUSTRIAL CLASSIFICATION:\tRETAIL-CATALOG & MAIL-ORDER HOUSES [5961]\n",
      "\t\tIRS NUMBER:\t\t\t\t911646860\n",
      "\t\tSTATE OF INCORPORATION:\t\t\tDE\n",
      "\t\tFISCAL YEAR END:\t\t\t1231\n",
      "\n",
      "\tFILING VALUES:\n",
      "\t\tFORM TYPE:\t\t10-K\n",
      "\t\tSEC ACT:\t\t1934 Act\n",
      "\t\tSEC FILE NUMBER:\t000-22513\n",
      "\t\tFILM NUMBER:\t\t20562951\n",
      "\n",
      "\tBUSINESS ADDRESS:\t\n",
      "\t\tSTREET 1:\t\t410 TERRY AVENUE NORTH\n",
      "\t\tCITY:\t\t\tSEATTLE\n",
      "\t\tSTATE:\t\t\tWA\n",
      "\t\tZIP:\t\t\t98109\n",
      "\t\tBUSINESS PHONE:\t\t2062661000\n",
      "\n",
      "\tMAIL ADDRESS:\t\n",
      "\t\tSTREET 1:\t\t410 TERRY AVENUE NORTH\n",
      "\t\tCITY:\t\t\tSEATTLE\n",
      "\t\tSTATE:\t\t\tWA\n",
      "\t\tZIP:\t\t\t98109\n",
      "</SEC-HEADER>\n",
      "<DOCUMENT>\n",
      "<TYPE>10-K\n",
      "<SEQUENCE>1\n",
      "<FILENAM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download fillings from the urls we get in last step\n",
    "raw_fillings_by_ticker = {}\n",
    "\n",
    "for ticker, data in sec_data.items():\n",
    "    raw_fillings_by_ticker[ticker] = {}\n",
    "    for index_url, file_type, file_date in tqdm(data, desc='Downloading {} Fillings'.format(ticker), unit='filling'):\n",
    "        if (file_type == '10-K'):\n",
    "            file_url = index_url.replace('-index.htm', '.txt').replace('.txtl', '.txt')            \n",
    "            \n",
    "            raw_fillings_by_ticker[ticker][file_date] = sec_api.get(file_url)\n",
    "\n",
    "\n",
    "print('Example Document:\\n\\n{}...'.format(next(iter(raw_fillings_by_ticker[example_ticker].values()))[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get documents from the fillings\n",
    "import re\n",
    "\n",
    "# To return a list of documents from a filling\n",
    "def get_documents(text):\n",
    "    extracted_docs = []\n",
    "    \n",
    "    doc_start_pattern = re.compile(r'<DOCUMENT>')\n",
    "    doc_end_pattern = re.compile(r'</DOCUMENT>')   \n",
    "    \n",
    "    doc_start_is = [x.end() for x in doc_start_pattern.finditer(text)]\n",
    "    doc_end_is = [x.start() for x in doc_end_pattern.finditer(text)]\n",
    "    \n",
    "    for doc_start_i, doc_end_i in zip(doc_start_is, doc_end_is):\n",
    "            extracted_docs.append(text[doc_start_i:doc_end_i])\n",
    "    \n",
    "    return extracted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Documents from AMZN Fillings: 100%|██████████| 20/20 [00:00<00:00, 78.28filling/s]\n",
      "Getting Documents from FB Fillings: 100%|██████████| 8/8 [00:00<00:00, 44.39filling/s]\n",
      "Getting Documents from GOOG Fillings: 100%|██████████| 5/5 [00:00<00:00, 35.12filling/s]\n",
      "Getting Documents from MSFT Fillings: 100%|██████████| 27/27 [00:00<00:00, 60.72filling/s]\n",
      "Getting Documents from TSLA Fillings: 100%|██████████| 10/10 [00:00<00:00, 32.21filling/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 Filed on 2020-01-31:\n",
      "\n",
      "<TYPE>10-K\n",
      "<SEQUENCE>1\n",
      "<FILENAME>amzn-20191231x10k.htm\n",
      "<DESCRIPTION>10-K\n",
      "<TEXT>\n",
      "<XBRL>\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<!--XBRL Document Created with Wdesk from Workiva-->\n",
      "<!--p:c57a17684e854b...\n",
      "\n",
      "Document 1 Filed on 2020-01-31:\n",
      "\n",
      "<TYPE>EX-4.6\n",
      "<SEQUENCE>2\n",
      "<FILENAME>amzn-20191231xex46.htm\n",
      "<DESCRIPTION>EXHIBIT 4.6\n",
      "<TEXT>\n",
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n",
      "<html>...\n",
      "\n",
      "Document 2 Filed on 2020-01-31:\n",
      "\n",
      "<TYPE>EX-21.1\n",
      "<SEQUENCE>3\n",
      "<FILENAME>amzn-20191231xex211.htm\n",
      "<DESCRIPTION>EXHIBIT 21.1\n",
      "<TEXT>\n",
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n",
      "<ht...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the documents for the company\n",
    "filling_documents_by_ticker = {}\n",
    "\n",
    "for ticker, raw_fillings in raw_fillings_by_ticker.items():\n",
    "    filling_documents_by_ticker[ticker] = {}\n",
    "    for file_date, filling in tqdm(raw_fillings.items(), desc='Getting Documents from {} Fillings'.format(ticker), unit='filling'):\n",
    "        filling_documents_by_ticker[ticker][file_date] = get_documents(filling)\n",
    "\n",
    "\n",
    "print('\\n\\n'.join([\n",
    "    'Document {} Filed on {}:\\n{}...'.format(doc_i, file_date, doc[:200])\n",
    "    for file_date, docs in filling_documents_by_ticker[example_ticker].items()\n",
    "    for doc_i, doc in enumerate(docs)][:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to get documents according to the type\n",
    "def get_document_type(doc):\n",
    "    type_pattern = re.compile(r'<TYPE>[^\\n]+')\n",
    "    \n",
    "    doc_type = type_pattern.findall(doc)[0][len('<TYPE>'):] \n",
    "    \n",
    "    return doc_type.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    cik: '0001018724'\n",
      "    file: '\\n<TYPE>10-K\\n<SEQUENCE>1\\n<FILENAME>amzn-2019123...\n",
      "    file_date: '2020-01-31'},\n",
      "  {\n",
      "    cik: '0001018724'\n",
      "    file: '\\n<TYPE>10-K\\n<SEQUENCE>1\\n<FILENAME>amzn-2018123...\n",
      "    file_date: '2019-02-01'},\n",
      "  {\n",
      "    cik: '0001018724'\n",
      "    file: '\\n<TYPE>10-K\\n<SEQUENCE>1\\n<FILENAME>amzn-2017123...\n",
      "    file_date: '2018-02-02'},\n",
      "  {\n",
      "    cik: '0001018724'\n",
      "    file: '\\n<TYPE>10-K\\n<SEQUENCE>1\\n<FILENAME>amzn-2016123...\n",
      "    file_date: '2017-02-10'},\n",
      "  {\n",
      "    cik: '0001018724'\n",
      "    file: '\\n<TYPE>10-K\\n<SEQUENCE>1\\n<FILENAME>amzn-2015123...\n",
      "    file_date: '2016-01-29'},\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Only get the 10-k documents for the companies we selected\n",
    "ten_ks_by_ticker = {}\n",
    "\n",
    "for ticker, filling_documents in filling_documents_by_ticker.items():\n",
    "    ten_ks_by_ticker[ticker] = []\n",
    "    for file_date, documents in filling_documents.items():\n",
    "        for document in documents:\n",
    "            if get_document_type(document) == '10-k':\n",
    "                ten_ks_by_ticker[ticker].append({\n",
    "                    'cik': cik_lookup[ticker],\n",
    "                    'file': document,\n",
    "                    'file_date': file_date})\n",
    "\n",
    "\n",
    "project_helper.print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['cik', 'file', 'file_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up - remove the html tags and lowercase all the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = remove_html_tags(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning AMZN 10-Ks: 100%|██████████| 20/20 [00:31<00:00,  1.56s/10-K]\n",
      "Cleaning FB 10-Ks: 100%|██████████| 8/8 [00:12<00:00,  1.60s/10-K]\n",
      "Cleaning GOOG 10-Ks: 100%|██████████| 5/5 [00:10<00:00,  2.08s/10-K]\n",
      "Cleaning MSFT 10-Ks: 100%|██████████| 27/27 [00:40<00:00,  1.49s/10-K]\n",
      "Cleaning TSLA 10-Ks: 100%|██████████| 10/10 [00:18<00:00,  1.82s/10-K]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    file_clean: '\\n10-k\\n1\\namzn-20191231x10k.htm\\n10-k\\n\\n\\n\\n\\n\\...},\n",
      "  {\n",
      "    file_clean: '\\n10-k\\n1\\namzn-20181231x10k.htm\\n10-k\\n\\n\\n\\n\\n\\...},\n",
      "  {\n",
      "    file_clean: '\\n10-k\\n1\\namzn-20171231x10k.htm\\n10-k\\n\\n\\n\\n\\n\\...},\n",
      "  {\n",
      "    file_clean: '\\n10-k\\n1\\namzn-20161231x10k.htm\\nform 10-k\\n\\n\\n...},\n",
      "  {\n",
      "    file_clean: '\\n10-k\\n1\\namzn-20151231x10k.htm\\nform 10-k\\n\\n\\n...},\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "    for ten_k in tqdm(ten_ks, desc='Cleaning {} 10-Ks'.format(ticker), unit='10-K'):\n",
    "        ten_k['file_clean'] = clean_text(ten_k['file'])\n",
    "\n",
    "project_helper.print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['file_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize\n",
    "Grouping together various inflections of a word to analyze them as a single item, identified by the word’s lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    lemmatized_words = [WordNetLemmatizer().lemmatize(word, 'v') for word in words]\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lemmatize AMZN 10-Ks: 100%|██████████| 20/20 [00:04<00:00,  4.2710-K/s]\n",
      "Lemmatize FB 10-Ks: 100%|██████████| 8/8 [00:01<00:00,  4.8810-K/s]\n",
      "Lemmatize GOOG 10-Ks: 100%|██████████| 5/5 [00:01<00:00,  4.8110-K/s]\n",
      "Lemmatize MSFT 10-Ks: 100%|██████████| 27/27 [00:03<00:00,  6.8910-K/s]\n",
      "Lemmatize TSLA 10-Ks: 100%|██████████| 10/10 [00:03<00:00,  3.1110-K/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    file_lemma: '['10', 'k', '1', 'amzn', '20191231x10k', 'htm', '...},\n",
      "  {\n",
      "    file_lemma: '['10', 'k', '1', 'amzn', '20181231x10k', 'htm', '...},\n",
      "  {\n",
      "    file_lemma: '['10', 'k', '1', 'amzn', '20171231x10k', 'htm', '...},\n",
      "  {\n",
      "    file_lemma: '['10', 'k', '1', 'amzn', '20161231x10k', 'htm', '...},\n",
      "  {\n",
      "    file_lemma: '['10', 'k', '1', 'amzn', '20151231x10k', 'htm', '...},\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word_pattern = re.compile('\\w+')\n",
    "\n",
    "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "    for ten_k in tqdm(ten_ks, desc='Lemmatize {} 10-Ks'.format(ticker), unit='10-K'):\n",
    "        ten_k['file_lemma'] = lemmatize_words(word_pattern.findall(ten_k['file_clean']))\n",
    "\n",
    "\n",
    "project_helper.print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['file_lemma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Remove Stop Words for AMZN 10-Ks: 100%|██████████| 20/20 [00:01<00:00, 18.0610-K/s]\n",
      "Remove Stop Words for FB 10-Ks: 100%|██████████| 8/8 [00:00<00:00, 14.7210-K/s]\n",
      "Remove Stop Words for GOOG 10-Ks: 100%|██████████| 5/5 [00:00<00:00, 13.0210-K/s]\n",
      "Remove Stop Words for MSFT 10-Ks: 100%|██████████| 27/27 [00:01<00:00, 19.3910-K/s]\n",
      "Remove Stop Words for TSLA 10-Ks: 100%|██████████| 10/10 [00:01<00:00,  8.3510-K/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemma_english_stopwords = lemmatize_words(stopwords.words('english'))\n",
    "\n",
    "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "    for ten_k in tqdm(ten_ks, desc='Remove Stop Words for {} 10-Ks'.format(ticker), unit='10-K'):\n",
    "        ten_k['file_lemma'] = [word for word in ten_k['file_lemma'] if word not in lemma_english_stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the keys for each ten_k is ['cik', 'file', 'file_date', 'file_clean', 'file_lemma']."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the Data Format - from Dict to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>company</th>\n",
       "      <th>ticker</th>\n",
       "      <th>text</th>\n",
       "      <th>doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0001018724</td>\n",
       "      <td>[10, k, 1, amzn, 20191231x10k, htm, 10, k, doc...</td>\n",
       "      <td>10 k 1 amzn 20191231x10k htm 10 k document 0 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0001018724</td>\n",
       "      <td>[10, k, 1, amzn, 20181231x10k, htm, 10, k, doc...</td>\n",
       "      <td>10 k 1 amzn 20181231x10k htm 10 k document tab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0001018724</td>\n",
       "      <td>[10, k, 1, amzn, 20171231x10k, htm, 10, k, doc...</td>\n",
       "      <td>10 k 1 amzn 20171231x10k htm 10 k document uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-02-10</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0001018724</td>\n",
       "      <td>[10, k, 1, amzn, 20161231x10k, htm, form, 10, ...</td>\n",
       "      <td>10 k 1 amzn 20161231x10k htm form 10 k documen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-29</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0001018724</td>\n",
       "      <td>[10, k, 1, amzn, 20151231x10k, htm, form, 10, ...</td>\n",
       "      <td>10 k 1 amzn 20151231x10k htm form 10 k 10 k un...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date company      ticker  \\\n",
       "0  2020-01-31    AMZN  0001018724   \n",
       "1  2019-02-01    AMZN  0001018724   \n",
       "2  2018-02-02    AMZN  0001018724   \n",
       "3  2017-02-10    AMZN  0001018724   \n",
       "4  2016-01-29    AMZN  0001018724   \n",
       "\n",
       "                                                text  \\\n",
       "0  [10, k, 1, amzn, 20191231x10k, htm, 10, k, doc...   \n",
       "1  [10, k, 1, amzn, 20181231x10k, htm, 10, k, doc...   \n",
       "2  [10, k, 1, amzn, 20171231x10k, htm, 10, k, doc...   \n",
       "3  [10, k, 1, amzn, 20161231x10k, htm, form, 10, ...   \n",
       "4  [10, k, 1, amzn, 20151231x10k, htm, form, 10, ...   \n",
       "\n",
       "                                                 doc  \n",
       "0  10 k 1 amzn 20191231x10k htm 10 k document 0 5...  \n",
       "1  10 k 1 amzn 20181231x10k htm 10 k document tab...  \n",
       "2  10 k 1 amzn 20171231x10k htm 10 k document uni...  \n",
       "3  10 k 1 amzn 20161231x10k htm form 10 k documen...  \n",
       "4  10 k 1 amzn 20151231x10k htm form 10 k 10 k un...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten_ks_df_dict = {'date': [], 'company': [], 'ticker': [], 'lemma': [], 'doc': []}\n",
    "\n",
    "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "    for ten_k in ten_ks:\n",
    "        ten_ks_df_dict['date'].append(ten_k['file_date'])\n",
    "        ten_ks_df_dict['company'].append(ticker)\n",
    "        ten_ks_df_dict['ticker'].append(cik_lookup[ticker])\n",
    "        ten_ks_df_dict['lemma'].append(ten_k['file_lemma'])\n",
    "        ten_ks_df_dict['doc'].append(' '.join(ten_k['file_lemma']))\n",
    "\n",
    "ten_ks_df = pd.DataFrame(ten_ks_df_dict)\n",
    "\n",
    "ten_ks_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each company will only have one financial report each year, we will consider the 10ks text data is the same in the same year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>company</th>\n",
       "      <th>ticker</th>\n",
       "      <th>text</th>\n",
       "      <th>doc</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0001018724</td>\n",
       "      <td>[10, k, 1, amzn, 20191231x10k, htm, 10, k, doc...</td>\n",
       "      <td>10 k 1 amzn 20191231x10k htm 10 k document 0 5...</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0001018724</td>\n",
       "      <td>[10, k, 1, amzn, 20181231x10k, htm, 10, k, doc...</td>\n",
       "      <td>10 k 1 amzn 20181231x10k htm 10 k document tab...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0001018724</td>\n",
       "      <td>[10, k, 1, amzn, 20171231x10k, htm, 10, k, doc...</td>\n",
       "      <td>10 k 1 amzn 20171231x10k htm 10 k document uni...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-02-10</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0001018724</td>\n",
       "      <td>[10, k, 1, amzn, 20161231x10k, htm, form, 10, ...</td>\n",
       "      <td>10 k 1 amzn 20161231x10k htm form 10 k documen...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-29</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0001018724</td>\n",
       "      <td>[10, k, 1, amzn, 20151231x10k, htm, form, 10, ...</td>\n",
       "      <td>10 k 1 amzn 20151231x10k htm form 10 k 10 k un...</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date company      ticker  \\\n",
       "0  2020-01-31    AMZN  0001018724   \n",
       "1  2019-02-01    AMZN  0001018724   \n",
       "2  2018-02-02    AMZN  0001018724   \n",
       "3  2017-02-10    AMZN  0001018724   \n",
       "4  2016-01-29    AMZN  0001018724   \n",
       "\n",
       "                                                text  \\\n",
       "0  [10, k, 1, amzn, 20191231x10k, htm, 10, k, doc...   \n",
       "1  [10, k, 1, amzn, 20181231x10k, htm, 10, k, doc...   \n",
       "2  [10, k, 1, amzn, 20171231x10k, htm, 10, k, doc...   \n",
       "3  [10, k, 1, amzn, 20161231x10k, htm, form, 10, ...   \n",
       "4  [10, k, 1, amzn, 20151231x10k, htm, form, 10, ...   \n",
       "\n",
       "                                                 doc  year  \n",
       "0  10 k 1 amzn 20191231x10k htm 10 k document 0 5...  2020  \n",
       "1  10 k 1 amzn 20181231x10k htm 10 k document tab...  2019  \n",
       "2  10 k 1 amzn 20171231x10k htm 10 k document uni...  2018  \n",
       "3  10 k 1 amzn 20161231x10k htm form 10 k documen...  2017  \n",
       "4  10 k 1 amzn 20151231x10k htm form 10 k 10 k un...  2016  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten_ks_df['year'] = ten_ks_df['date'].apply(lambda x: x.split('-')[0])\n",
    "\n",
    "ten_ks_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Loughran-McDonald sentiment word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>uncertainty</th>\n",
       "      <th>litigious</th>\n",
       "      <th>constraining</th>\n",
       "      <th>interesting</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>abandon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>abandonment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>abandonments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>abdicate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>abdication</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    negative  positive  uncertainty  litigious  constraining  interesting  \\\n",
       "9       True     False        False      False         False        False   \n",
       "12      True     False        False      False         False        False   \n",
       "13      True     False        False      False         False        False   \n",
       "51      True     False        False      False         False        False   \n",
       "54      True     False        False      False         False        False   \n",
       "\n",
       "            word  \n",
       "9        abandon  \n",
       "12   abandonment  \n",
       "13  abandonments  \n",
       "51      abdicate  \n",
       "54    abdication  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments = ['negative', 'positive', 'uncertainty', 'litigious', 'constraining', 'interesting']\n",
    "\n",
    "# Read the file\n",
    "sentiment_df = pd.read_csv('LoughranMcDonald_MasterDictionary_2018.csv')\n",
    "sentiment_df.columns = [column.lower() for column in sentiment_df.columns]\n",
    "\n",
    "# Remove unused information\n",
    "sentiment_df = sentiment_df[sentiments + ['word']]\n",
    "sentiment_df[sentiments] = sentiment_df[sentiments].astype(bool)\n",
    "sentiment_df = sentiment_df[(sentiment_df[sentiments]).any(1)]\n",
    "\n",
    "# Apply the same preprocessing to these words as the 10-k words\n",
    "sentiment_df['word'] = lemmatize_words(sentiment_df['word'].str.lower())\n",
    "sentiment_df = sentiment_df.drop_duplicates('word')\n",
    "\n",
    "sentiment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the positive words and negative words as vocabulary \n",
    "positive_voc = sentiment_df[sentiment_df['positive']]['word']\n",
    "negative_voc = sentiment_df[sentiment_df['negative']]['word']\n",
    "\n",
    "voc = positive_voc.append(negative_voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the 10ks documents to tf-idf vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=125                 able\n",
       "336            abundance\n",
       "338             abundant\n",
       "438              acclaim\n",
       "477           accomplish\n",
       "481       accomplishment\n",
       "482      accomplishments\n",
       "620              achieve\n",
       "623          achievement\n",
       "624         achievements\n",
       "906           adequately\n",
       "1137         a...  wrongful\n",
       "86041         wrongfully\n",
       "86049            wrongly\n",
       "Name: word, Length: 1764, dtype: object)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the tfidf vectorizer based on the vocabulary\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer(vocabulary=voc)\n",
    "vec.fit(ten_ks_df['doc'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the tfidf sparse matrix for all the input 10k docs\n",
    "\n",
    "tfidf = vec.transform(ten_ks_df['doc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08153187 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.10263492 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.09360538 0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.24585583 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.29710243 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.27459558 0.         0.01493778 ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Generate TFIDF values from documents for a certain sentiment\n",
    "def get_tfidf(sentiment_words, docs):\n",
    "    \n",
    "    vec = TfidfVectorizer(vocabulary=sentiment_words)\n",
    "    tfidf = vec.fit_transform(docs)\n",
    "    \n",
    "    return tfidf.toarray()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    negative: '[[0.         0.         0.         ... 0.        ...\n",
      "    positive: '[[0.19672975 0.         0.         ... 0.        ...\n",
      "    uncertainty: '[[0.         0.         0.         ... 0.00570501...\n",
      "    litigious: '[[0. 0. 0. ... 0. 0. 0.]\\n [0. 0. 0. ... 0. 0. 0....\n",
      "    constraining: '[[0.         0.         0.         ... 0.        ...\n",
      "    interesting: '[[0.01889287 0.         0.         ... 0.        ...},\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "sentiment_tfidf_ten_ks = {}\n",
    "\n",
    "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "    lemma_docs = [' '.join(ten_k['file_lemma']) for ten_k in ten_ks]\n",
    "    if not lemma_docs:\n",
    "        print('got')\n",
    "        break\n",
    "    \n",
    "    sentiment_tfidf_ten_ks[ticker] = {\n",
    "        sentiment: get_tfidf(sentiment_df[sentiment_df[sentiment]]['word'], lemma_docs)\n",
    "        for sentiment in sentiments}\n",
    "\n",
    "    \n",
    "project_helper.print_ten_k_data([sentiment_tfidf_ten_ks[example_ticker]], sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=9                abandon\n",
       "12           abandonment\n",
       "13          abandonments\n",
       "51              abdicate\n",
       "54            abdication\n",
       "55           abdications\n",
       "70              aberrant\n",
       "71            aberration\n",
       "72          aberrational\n",
       "73           aberrations\n",
       "79                  abet\n",
       "138           ...  wrongful\n",
       "86041         wrongfully\n",
       "86049            wrongly\n",
       "Name: word, Length: 1515, dtype: object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_voc = sentiment_df[sentiment_df['positive']]['word']\n",
    "negative_voc = sentiment_df[sentiment_df['negative']]['word']\n",
    "\n",
    "positive_vec = TfidfVectorizer(vocabulary=positive_voc)\n",
    "positive_vec.fit(ten_ks_df['doc'].values)\n",
    "negative_vec = TfidfVectorizer(vocabulary=negative_voc)\n",
    "negative_vec.fit(ten_ks_df['doc'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(vec, docs):\n",
    "    \n",
    "    # vec = TfidfVectorizer(vocabulary=sentiment_words)\n",
    "    vec.fit(ten_ks_df['doc'].values())\n",
    "    tfidf = fit_transform(docs)\n",
    "    \n",
    "    #return tfidf.toarray()\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tfidf = positive_vec.transform(ten_ks_df['doc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.18780437 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.2291988  0.         0.         ... 0.         0.         0.        ]\n",
      " [0.20790583 0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.49683789 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.55671838 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.47896312 0.         0.02605521 ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "test = pos_tfidf.toarray()\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = pd.DataFrame(pos_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.187804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.229199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.207906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.212847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043433</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.228727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 249 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1    2    3    4    5    6         7    8    9    ...  239  240  \\\n",
       "0  0.187804  0.0  0.0  0.0  0.0  0.0  0.0  0.068129  0.0  0.0  ...  0.0  0.0   \n",
       "1  0.229199  0.0  0.0  0.0  0.0  0.0  0.0  0.066516  0.0  0.0  ...  0.0  0.0   \n",
       "2  0.207906  0.0  0.0  0.0  0.0  0.0  0.0  0.060337  0.0  0.0  ...  0.0  0.0   \n",
       "3  0.212847  0.0  0.0  0.0  0.0  0.0  0.0  0.043433  0.0  0.0  ...  0.0  0.0   \n",
       "4  0.228727  0.0  0.0  0.0  0.0  0.0  0.0  0.046673  0.0  0.0  ...  0.0  0.0   \n",
       "\n",
       "   241  242  243  244  245  246  247  248  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 249 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Data must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-ffd09f9d173f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mten_ks_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pos_tfidf'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Anaconda3/anaconda/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 data = sanitize_array(data, index, dtype, copy,\n\u001b[0;32m--> 262\u001b[0;31m                                       raise_cast_failure=True)\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Anaconda3/anaconda/lib/python3.6/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data must be 1-dimensional'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_tuplesafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Data must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "ten_ks_df['pos_tfidf'] = pd.Series(pos_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
