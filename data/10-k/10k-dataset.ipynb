{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10K Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pprint\n",
    "import project_helper\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download NLP Corpora\n",
    "Use the stopwords and wordnet from nltk to preprocess the dataset later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/kathy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/kathy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get 10Ks Dataset\n",
    "We will use the company's unique CIK (Central Index Key) to identify each company.  \n",
    "Find the CIK through: https://www.sec.gov/edgar/searchedgar/companysearch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the company CIK here\n",
    "cik_lookup = {\n",
    "    'AMZN': '0001018724',\n",
    "    'FB': '0001326801',   \n",
    "    'GOOG': '0001652044',\n",
    "    'MSFT': '0000789019',\n",
    "    'TSLA': '0001318605'}\n",
    "\n",
    "additional_cik = {\n",
    "    'AEP': '0000004904',\n",
    "    'AXP': '0000004962',\n",
    "    'BA': '0000012927', \n",
    "    'BK': '0001390777',\n",
    "    'CAT': '0000018230',\n",
    "    'DE': '0000315189',\n",
    "    'DIS': '0001001039',\n",
    "    'DTE': '0000936340',\n",
    "    'ED': '0001047862',\n",
    "    'EMR': '0000032604',\n",
    "    'ETN': '0001551182',\n",
    "    'GE': '0000040545',\n",
    "    'IBM': '0000051143',\n",
    "    'IP': '0000051434',\n",
    "    'JNJ': '0000200406',\n",
    "    'KO': '0000021344',\n",
    "    'LLY': '0000059478',\n",
    "    'MCD': '0000063908',\n",
    "    'MO': '0000764180',\n",
    "    'MRK': '0000310158',\n",
    "    'MRO': '0000101778',\n",
    "    'PCG': '0001004980',\n",
    "    'PEP': '0000077476',\n",
    "    'PFE': '0000078003',\n",
    "    'PG': '0000080424',\n",
    "    'PNR': '0000077360',\n",
    "    'SYY': '0000096021',\n",
    "    'TXN': '0000097476',\n",
    "    'UTX': '0000101829',\n",
    "    'WFC': '0000072971',\n",
    "    'WMT': '0000104169',\n",
    "    'WY': '0000106535',\n",
    "    'XOM': '0000034088'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SecAPI to get the 10Ks data\n",
    "sec_api = project_helper.SecAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull a lost of filled 10-ks from the API for each company\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_sec_data(cik, doc_type, start=0, count=60):\n",
    "    rss_url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany' \\\n",
    "        '&CIK={}&type={}&start={}&count={}&owner=exclude&output=atom' \\\n",
    "        .format(cik, doc_type, start, count)\n",
    "    sec_data = sec_api.get(rss_url)\n",
    "    feed = BeautifulSoup(sec_data.encode('ascii'), 'xml').feed\n",
    "    entries = [\n",
    "        (\n",
    "            entry.content.find('filing-href').getText(),\n",
    "            entry.content.find('filing-type').getText(),\n",
    "            entry.content.find('filing-date').getText())\n",
    "        for entry in feed.find_all('entry', recursive=False)]\n",
    "\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https://www.sec.gov/Archives/edgar/data/1018724/000101872420000004/0001018724-20-000004-index.htm',\n",
      "  '10-K',\n",
      "  '2020-01-31'),\n",
      " ('https://www.sec.gov/Archives/edgar/data/1018724/000101872419000004/0001018724-19-000004-index.htm',\n",
      "  '10-K',\n",
      "  '2019-02-01'),\n",
      " ('https://www.sec.gov/Archives/edgar/data/1018724/000101872418000005/0001018724-18-000005-index.htm',\n",
      "  '10-K',\n",
      "  '2018-02-02'),\n",
      " ('https://www.sec.gov/Archives/edgar/data/1018724/000101872417000011/0001018724-17-000011-index.htm',\n",
      "  '10-K',\n",
      "  '2017-02-10'),\n",
      " ('https://www.sec.gov/Archives/edgar/data/1018724/000101872416000172/0001018724-16-000172-index.htm',\n",
      "  '10-K',\n",
      "  '2016-01-29')]\n"
     ]
    }
   ],
   "source": [
    "# Pull the data, and show one of the examples\n",
    "example_ticker = 'AMZN'\n",
    "sec_data = {}\n",
    "\n",
    "for ticker, cik in cik_lookup.items():\n",
    "    sec_data[ticker] = get_sec_data(cik, '10-K')\n",
    "\n",
    "pprint.pprint(sec_data[example_ticker][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading AMZN Fillings: 100%|██████████| 25/25 [00:06<00:00,  3.61filling/s]\n",
      "Downloading FB Fillings: 100%|██████████| 10/10 [00:03<00:00,  2.90filling/s]\n",
      "Downloading GOOG Fillings: 100%|██████████| 7/7 [00:02<00:00,  3.21filling/s]\n",
      "Downloading MSFT Fillings: 100%|██████████| 27/27 [00:09<00:00,  2.98filling/s]\n",
      "Downloading TSLA Fillings: 100%|██████████| 12/12 [00:04<00:00,  2.55filling/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Document:\n",
      "\n",
      "<SEC-DOCUMENT>0001018724-20-000004.txt : 20200131\n",
      "<SEC-HEADER>0001018724-20-000004.hdr.sgml : 20200131\n",
      "<ACCEPTANCE-DATETIME>20200130204613\n",
      "ACCESSION NUMBER:\t\t0001018724-20-000004\n",
      "CONFORMED SUBMISSION TYPE:\t10-K\n",
      "PUBLIC DOCUMENT COUNT:\t\t109\n",
      "CONFORMED PERIOD OF REPORT:\t20191231\n",
      "FILED AS OF DATE:\t\t20200131\n",
      "DATE AS OF CHANGE:\t\t20200130\n",
      "\n",
      "FILER:\n",
      "\n",
      "\tCOMPANY DATA:\t\n",
      "\t\tCOMPANY CONFORMED NAME:\t\t\tAMAZON COM INC\n",
      "\t\tCENTRAL INDEX KEY:\t\t\t0001018724\n",
      "\t\tSTANDARD INDUSTRIAL CLASSIFICATION:\tRETAIL-CATALOG & MAIL-ORDER HOUSES [5961]\n",
      "\t\tIRS NUMBER:\t\t\t\t911646860\n",
      "\t\tSTATE OF INCORPORATION:\t\t\tDE\n",
      "\t\tFISCAL YEAR END:\t\t\t1231\n",
      "\n",
      "\tFILING VALUES:\n",
      "\t\tFORM TYPE:\t\t10-K\n",
      "\t\tSEC ACT:\t\t1934 Act\n",
      "\t\tSEC FILE NUMBER:\t000-22513\n",
      "\t\tFILM NUMBER:\t\t20562951\n",
      "\n",
      "\tBUSINESS ADDRESS:\t\n",
      "\t\tSTREET 1:\t\t410 TERRY AVENUE NORTH\n",
      "\t\tCITY:\t\t\tSEATTLE\n",
      "\t\tSTATE:\t\t\tWA\n",
      "\t\tZIP:\t\t\t98109\n",
      "\t\tBUSINESS PHONE:\t\t2062661000\n",
      "\n",
      "\tMAIL ADDRESS:\t\n",
      "\t\tSTREET 1:\t\t410 TERRY AVENUE NORTH\n",
      "\t\tCITY:\t\t\tSEATTLE\n",
      "\t\tSTATE:\t\t\tWA\n",
      "\t\tZIP:\t\t\t98109\n",
      "</SEC-HEADER>\n",
      "<DOCUMENT>\n",
      "<TYPE>10-K\n",
      "<SEQUENCE>1\n",
      "<FILENAM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download fillings from the urls we get in last step\n",
    "raw_fillings_by_ticker = {}\n",
    "\n",
    "for ticker, data in sec_data.items():\n",
    "    raw_fillings_by_ticker[ticker] = {}\n",
    "    for index_url, file_type, file_date in tqdm(data, desc='Downloading {} Fillings'.format(ticker), unit='filling'):\n",
    "        if (file_type == '10-K'):\n",
    "            file_url = index_url.replace('-index.htm', '.txt').replace('.txtl', '.txt')            \n",
    "            \n",
    "            raw_fillings_by_ticker[ticker][file_date] = sec_api.get(file_url)\n",
    "\n",
    "\n",
    "print('Example Document:\\n\\n{}...'.format(next(iter(raw_fillings_by_ticker[example_ticker].values()))[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get documents from the fillings\n",
    "import re\n",
    "\n",
    "# To return a list of documents from a filling\n",
    "def get_documents(text):\n",
    "    extracted_docs = []\n",
    "    \n",
    "    doc_start_pattern = re.compile(r'<DOCUMENT>')\n",
    "    doc_end_pattern = re.compile(r'</DOCUMENT>')   \n",
    "    \n",
    "    doc_start_is = [x.end() for x in doc_start_pattern.finditer(text)]\n",
    "    doc_end_is = [x.start() for x in doc_end_pattern.finditer(text)]\n",
    "    \n",
    "    for doc_start_i, doc_end_i in zip(doc_start_is, doc_end_is):\n",
    "            extracted_docs.append(text[doc_start_i:doc_end_i])\n",
    "    \n",
    "    return extracted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Documents from AMZN Fillings: 100%|██████████| 20/20 [00:00<00:00, 75.73filling/s]\n",
      "Getting Documents from FB Fillings: 100%|██████████| 8/8 [00:00<00:00, 41.60filling/s]\n",
      "Getting Documents from GOOG Fillings: 100%|██████████| 5/5 [00:00<00:00, 31.93filling/s]\n",
      "Getting Documents from MSFT Fillings: 100%|██████████| 27/27 [00:00<00:00, 56.09filling/s]\n",
      "Getting Documents from TSLA Fillings: 100%|██████████| 10/10 [00:00<00:00, 29.18filling/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 Filed on 2020-01-31:\n",
      "\n",
      "<TYPE>10-K\n",
      "<SEQUENCE>1\n",
      "<FILENAME>amzn-20191231x10k.htm\n",
      "<DESCRIPTION>10-K\n",
      "<TEXT>\n",
      "<XBRL>\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<!--XBRL Document Created with Wdesk from Workiva-->\n",
      "<!--p:c57a17684e854b...\n",
      "\n",
      "Document 1 Filed on 2020-01-31:\n",
      "\n",
      "<TYPE>EX-4.6\n",
      "<SEQUENCE>2\n",
      "<FILENAME>amzn-20191231xex46.htm\n",
      "<DESCRIPTION>EXHIBIT 4.6\n",
      "<TEXT>\n",
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n",
      "<html>...\n",
      "\n",
      "Document 2 Filed on 2020-01-31:\n",
      "\n",
      "<TYPE>EX-21.1\n",
      "<SEQUENCE>3\n",
      "<FILENAME>amzn-20191231xex211.htm\n",
      "<DESCRIPTION>EXHIBIT 21.1\n",
      "<TEXT>\n",
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n",
      "<ht...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the documents for the company\n",
    "filling_documents_by_ticker = {}\n",
    "\n",
    "for ticker, raw_fillings in raw_fillings_by_ticker.items():\n",
    "    filling_documents_by_ticker[ticker] = {}\n",
    "    for file_date, filling in tqdm(raw_fillings.items(), desc='Getting Documents from {} Fillings'.format(ticker), unit='filling'):\n",
    "        filling_documents_by_ticker[ticker][file_date] = get_documents(filling)\n",
    "\n",
    "\n",
    "print('\\n\\n'.join([\n",
    "    'Document {} Filed on {}:\\n{}...'.format(doc_i, file_date, doc[:200])\n",
    "    for file_date, docs in filling_documents_by_ticker[example_ticker].items()\n",
    "    for doc_i, doc in enumerate(docs)][:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to get documents according to the type\n",
    "def get_document_type(doc):\n",
    "    type_pattern = re.compile(r'<TYPE>[^\\n]+')\n",
    "    \n",
    "    doc_type = type_pattern.findall(doc)[0][len('<TYPE>'):] \n",
    "    \n",
    "    return doc_type.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    cik: '0001018724'\n",
      "    file: '\\n<TYPE>10-K\\n<SEQUENCE>1\\n<FILENAME>amzn-2019123...\n",
      "    file_date: '2020-01-31'},\n",
      "  {\n",
      "    cik: '0001018724'\n",
      "    file: '\\n<TYPE>10-K\\n<SEQUENCE>1\\n<FILENAME>amzn-2018123...\n",
      "    file_date: '2019-02-01'},\n",
      "  {\n",
      "    cik: '0001018724'\n",
      "    file: '\\n<TYPE>10-K\\n<SEQUENCE>1\\n<FILENAME>amzn-2017123...\n",
      "    file_date: '2018-02-02'},\n",
      "  {\n",
      "    cik: '0001018724'\n",
      "    file: '\\n<TYPE>10-K\\n<SEQUENCE>1\\n<FILENAME>amzn-2016123...\n",
      "    file_date: '2017-02-10'},\n",
      "  {\n",
      "    cik: '0001018724'\n",
      "    file: '\\n<TYPE>10-K\\n<SEQUENCE>1\\n<FILENAME>amzn-2015123...\n",
      "    file_date: '2016-01-29'},\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Only get the 10-k documents for the companies we selected\n",
    "ten_ks_by_ticker = {}\n",
    "\n",
    "for ticker, filling_documents in filling_documents_by_ticker.items():\n",
    "    ten_ks_by_ticker[ticker] = []\n",
    "    for file_date, documents in filling_documents.items():\n",
    "        for document in documents:\n",
    "            if get_document_type(document) == '10-k':\n",
    "                ten_ks_by_ticker[ticker].append({\n",
    "                    'cik': cik_lookup[ticker],\n",
    "                    'file': document,\n",
    "                    'file_date': file_date})\n",
    "\n",
    "\n",
    "project_helper.print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['cik', 'file', 'file_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up - remove the html tags and lowercase all the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = remove_html_tags(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning AMZN 10-Ks: 100%|██████████| 20/20 [00:30<00:00,  1.50s/10-K]\n",
      "Cleaning FB 10-Ks: 100%|██████████| 8/8 [00:12<00:00,  1.59s/10-K]\n",
      "Cleaning GOOG 10-Ks: 100%|██████████| 5/5 [00:10<00:00,  2.03s/10-K]\n",
      "Cleaning MSFT 10-Ks: 100%|██████████| 27/27 [00:40<00:00,  1.49s/10-K]\n",
      "Cleaning TSLA 10-Ks: 100%|██████████| 10/10 [00:18<00:00,  1.82s/10-K]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    file_clean: '\\n10-k\\n1\\namzn-20191231x10k.htm\\n10-k\\n\\n\\n\\n\\n\\...},\n",
      "  {\n",
      "    file_clean: '\\n10-k\\n1\\namzn-20181231x10k.htm\\n10-k\\n\\n\\n\\n\\n\\...},\n",
      "  {\n",
      "    file_clean: '\\n10-k\\n1\\namzn-20171231x10k.htm\\n10-k\\n\\n\\n\\n\\n\\...},\n",
      "  {\n",
      "    file_clean: '\\n10-k\\n1\\namzn-20161231x10k.htm\\nform 10-k\\n\\n\\n...},\n",
      "  {\n",
      "    file_clean: '\\n10-k\\n1\\namzn-20151231x10k.htm\\nform 10-k\\n\\n\\n...},\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "    for ten_k in tqdm(ten_ks, desc='Cleaning {} 10-Ks'.format(ticker), unit='10-K'):\n",
    "        ten_k['file_clean'] = clean_text(ten_k['file'])\n",
    "\n",
    "project_helper.print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['file_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize\n",
    "Grouping together various inflections of a word to analyze them as a single item, identified by the word’s lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    lemmatized_words = [WordNetLemmatizer().lemmatize(word, 'v') for word in words]\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lemmatize AMZN 10-Ks: 100%|██████████| 20/20 [00:04<00:00,  4.3610-K/s]\n",
      "Lemmatize FB 10-Ks: 100%|██████████| 8/8 [00:01<00:00,  4.9410-K/s]\n",
      "Lemmatize GOOG 10-Ks: 100%|██████████| 5/5 [00:01<00:00,  4.7410-K/s]\n",
      "Lemmatize MSFT 10-Ks: 100%|██████████| 27/27 [00:03<00:00,  6.8410-K/s]\n",
      "Lemmatize TSLA 10-Ks: 100%|██████████| 10/10 [00:03<00:00,  3.0310-K/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    file_lemma: '['10', 'k', '1', 'amzn', '20191231x10k', 'htm', '...},\n",
      "  {\n",
      "    file_lemma: '['10', 'k', '1', 'amzn', '20181231x10k', 'htm', '...},\n",
      "  {\n",
      "    file_lemma: '['10', 'k', '1', 'amzn', '20171231x10k', 'htm', '...},\n",
      "  {\n",
      "    file_lemma: '['10', 'k', '1', 'amzn', '20161231x10k', 'htm', '...},\n",
      "  {\n",
      "    file_lemma: '['10', 'k', '1', 'amzn', '20151231x10k', 'htm', '...},\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word_pattern = re.compile('\\w+')\n",
    "\n",
    "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "    for ten_k in tqdm(ten_ks, desc='Lemmatize {} 10-Ks'.format(ticker), unit='10-K'):\n",
    "        ten_k['file_lemma'] = lemmatize_words(word_pattern.findall(ten_k['file_clean']))\n",
    "\n",
    "\n",
    "project_helper.print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['file_lemma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Remove Stop Words for AMZN 10-Ks: 100%|██████████| 20/20 [00:01<00:00, 18.0010-K/s]\n",
      "Remove Stop Words for FB 10-Ks: 100%|██████████| 8/8 [00:00<00:00, 14.8810-K/s]\n",
      "Remove Stop Words for GOOG 10-Ks: 100%|██████████| 5/5 [00:00<00:00, 14.1910-K/s]\n",
      "Remove Stop Words for MSFT 10-Ks: 100%|██████████| 27/27 [00:01<00:00, 19.8910-K/s]\n",
      "Remove Stop Words for TSLA 10-Ks: 100%|██████████| 10/10 [00:01<00:00,  8.8310-K/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemma_english_stopwords = lemmatize_words(stopwords.words('english'))\n",
    "\n",
    "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "    for ten_k in tqdm(ten_ks, desc='Remove Stop Words for {} 10-Ks'.format(ticker), unit='10-K'):\n",
    "        ten_k['file_lemma'] = [word for word in ten_k['file_lemma'] if word not in lemma_english_stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the keys for each ten_k is ['cik', 'file', 'file_date', 'file_clean', 'file_lemma']."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the Data Format - from Dict to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>company</th>\n",
       "      <th>ticker</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0001018724</td>\n",
       "      <td>[10, k, 1, amzn, 20191231x10k, htm, 10, k, doc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0001018724</td>\n",
       "      <td>[10, k, 1, amzn, 20181231x10k, htm, 10, k, doc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0001018724</td>\n",
       "      <td>[10, k, 1, amzn, 20171231x10k, htm, 10, k, doc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-02-10</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0001018724</td>\n",
       "      <td>[10, k, 1, amzn, 20161231x10k, htm, form, 10, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-29</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0001018724</td>\n",
       "      <td>[10, k, 1, amzn, 20151231x10k, htm, form, 10, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date company      ticker  \\\n",
       "0  2020-01-31    AMZN  0001018724   \n",
       "1  2019-02-01    AMZN  0001018724   \n",
       "2  2018-02-02    AMZN  0001018724   \n",
       "3  2017-02-10    AMZN  0001018724   \n",
       "4  2016-01-29    AMZN  0001018724   \n",
       "\n",
       "                                                text  \n",
       "0  [10, k, 1, amzn, 20191231x10k, htm, 10, k, doc...  \n",
       "1  [10, k, 1, amzn, 20181231x10k, htm, 10, k, doc...  \n",
       "2  [10, k, 1, amzn, 20171231x10k, htm, 10, k, doc...  \n",
       "3  [10, k, 1, amzn, 20161231x10k, htm, form, 10, ...  \n",
       "4  [10, k, 1, amzn, 20151231x10k, htm, form, 10, ...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten_ks_df_dict = {'date': [], 'company': [], 'ticker': [], 'text': []}\n",
    "\n",
    "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "    for ten_k in ten_ks:\n",
    "        ten_ks_df_dict['date'].append(ten_k['file_date'])\n",
    "        ten_ks_df_dict['company'].append(ticker)\n",
    "        ten_ks_df_dict['ticker'].append(cik_lookup[ticker])\n",
    "        ten_ks_df_dict['text'].append(ten_k['file_lemma'])\n",
    "\n",
    "ten_ks_df = pd.DataFrame(ten_ks_df_dict)\n",
    "\n",
    "ten_ks_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each company will only have one financial report each year, we will consider the 10ks text data is the same in the same year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>company</th>\n",
       "      <th>ticker</th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0001018724</td>\n",
       "      <td>[10, k, 1, amzn, 20191231x10k, htm, 10, k, doc...</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0001018724</td>\n",
       "      <td>[10, k, 1, amzn, 20181231x10k, htm, 10, k, doc...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0001018724</td>\n",
       "      <td>[10, k, 1, amzn, 20171231x10k, htm, 10, k, doc...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-02-10</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0001018724</td>\n",
       "      <td>[10, k, 1, amzn, 20161231x10k, htm, form, 10, ...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-29</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0001018724</td>\n",
       "      <td>[10, k, 1, amzn, 20151231x10k, htm, form, 10, ...</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date company      ticker  \\\n",
       "0  2020-01-31    AMZN  0001018724   \n",
       "1  2019-02-01    AMZN  0001018724   \n",
       "2  2018-02-02    AMZN  0001018724   \n",
       "3  2017-02-10    AMZN  0001018724   \n",
       "4  2016-01-29    AMZN  0001018724   \n",
       "\n",
       "                                                text  year  \n",
       "0  [10, k, 1, amzn, 20191231x10k, htm, 10, k, doc...  2020  \n",
       "1  [10, k, 1, amzn, 20181231x10k, htm, 10, k, doc...  2019  \n",
       "2  [10, k, 1, amzn, 20171231x10k, htm, 10, k, doc...  2018  \n",
       "3  [10, k, 1, amzn, 20161231x10k, htm, form, 10, ...  2017  \n",
       "4  [10, k, 1, amzn, 20151231x10k, htm, form, 10, ...  2016  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten_ks_df['year'] = ten_ks_df['date'].apply(lambda x: x.split('-')[0])\n",
    "\n",
    "ten_ks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
