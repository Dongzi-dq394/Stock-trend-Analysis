{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTy8XvJMvgm-"
      },
      "source": [
        "# Get 10ks data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weQYFdPpwPct",
        "outputId": "07d350a5-86ce-40c3-91a6-39e470507d4a"
      },
      "source": [
        "pip install ratelimit"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ratelimit in /usr/local/lib/python3.6/dist-packages (2.2.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J36EAKSN-50",
        "outputId": "b710b7ce-2d42-4df6-b3b6-30e513b71517"
      },
      "source": [
        "pip install w3lib"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: w3lib in /usr/local/lib/python3.6/dist-packages (1.22.0)\n",
            "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from w3lib) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghJFl9KqvfXv"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import pprint\n",
        "import project_helper\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTqVtvWYzhZP",
        "outputId": "e54855d4-d2df-47f9-f7fb-b2cd57ea5131"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "hEgwrZx9vgCq",
        "outputId": "1fa6f74c-c7cd-4d8a-84d5-4b2e9d204767"
      },
      "source": [
        "cik_df= pd.read_csv('cik_ticker.csv', sep=\"|\")\n",
        "cik_df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CIK</th>\n",
              "      <th>Ticker</th>\n",
              "      <th>Name</th>\n",
              "      <th>Exchange</th>\n",
              "      <th>SIC</th>\n",
              "      <th>Business</th>\n",
              "      <th>Incorporated</th>\n",
              "      <th>IRS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1090872</td>\n",
              "      <td>A</td>\n",
              "      <td>Agilent Technologies Inc</td>\n",
              "      <td>NYSE</td>\n",
              "      <td>3825.0</td>\n",
              "      <td>CA</td>\n",
              "      <td>DE</td>\n",
              "      <td>770518772.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4281</td>\n",
              "      <td>AA</td>\n",
              "      <td>Alcoa Inc</td>\n",
              "      <td>NYSE</td>\n",
              "      <td>3350.0</td>\n",
              "      <td>PA</td>\n",
              "      <td>PA</td>\n",
              "      <td>250317820.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1332552</td>\n",
              "      <td>AAACU</td>\n",
              "      <td>Asia Automotive Acquisition Corp</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6770.0</td>\n",
              "      <td>DE</td>\n",
              "      <td>DE</td>\n",
              "      <td>203022522.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1287145</td>\n",
              "      <td>AABB</td>\n",
              "      <td>Asia Broadband Inc</td>\n",
              "      <td>OTC</td>\n",
              "      <td>8200.0</td>\n",
              "      <td>GA</td>\n",
              "      <td>NV</td>\n",
              "      <td>721569126.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1024015</td>\n",
              "      <td>AABC</td>\n",
              "      <td>Access Anytime Bancorp Inc</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6035.0</td>\n",
              "      <td>NM</td>\n",
              "      <td>DE</td>\n",
              "      <td>850444597.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       CIK Ticker  ... Incorporated          IRS\n",
              "0  1090872      A  ...           DE  770518772.0\n",
              "1     4281     AA  ...           PA  250317820.0\n",
              "2  1332552  AAACU  ...           DE  203022522.0\n",
              "3  1287145   AABB  ...           NV  721569126.0\n",
              "4  1024015   AABC  ...           DE  850444597.0\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_ggmVK3vgFB"
      },
      "source": [
        "cik_map = {}\n",
        "\n",
        "for i in range(len(cik_df)):\n",
        "    cik_map[cik_df.iloc[i]['Ticker']] = cik_df.iloc[i]['CIK']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "e7l-OtwcvgHl",
        "outputId": "e594a6af-909f-4170-bbf8-79a7d593f7e8"
      },
      "source": [
        "company_df = pd.read_csv('dis_comp.csv')\n",
        "\n",
        "company_df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Company</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AAMC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AAME</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AAN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AAOI</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Company\n",
              "0       A\n",
              "1    AAMC\n",
              "2    AAME\n",
              "3     AAN\n",
              "4    AAOI"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDgrVPmkvgJ_"
      },
      "source": [
        "all_comp = []\n",
        "\n",
        "for i in range(len(company_df)):\n",
        "  all_comp.append(company_df.iloc[i]['Company'])\n",
        "\n",
        "company = all_comp[540:610]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivyo0S4X3Y_o",
        "outputId": "9469c6cb-ab9f-486e-891f-a2d939d82230"
      },
      "source": [
        "print(len(all_comp))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08KvU0MmvgMW"
      },
      "source": [
        "cik_lookup = {}\n",
        "\n",
        "for i, comp in enumerate(company):\n",
        "  if comp in cik_map.keys():\n",
        "    cik_lookup[comp] = cik_map[comp]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FXhbbzCvgOZ"
      },
      "source": [
        "# Use SecAPI to get the 10Ks data\n",
        "sec_api = project_helper.SecAPI()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UswmpDquxWuO"
      },
      "source": [
        "# Pull a lost of filled 10-ks from the API for each company\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_sec_data(cik, doc_type, start=0, count=60):\n",
        "    rss_url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany' \\\n",
        "        '&CIK={}&type={}&start={}&count={}&owner=exclude&output=atom' \\\n",
        "        .format(cik, doc_type, start, count)\n",
        "    sec_data = sec_api.get(rss_url)\n",
        "    feed = BeautifulSoup(sec_data.encode('ascii'), 'xml').feed\n",
        "    entries = [\n",
        "        (\n",
        "            entry.content.find('filing-href').getText(),\n",
        "            entry.content.find('filing-type').getText(),\n",
        "            entry.content.find('filing-date').getText())\n",
        "        for entry in feed.find_all('entry', recursive=False)]\n",
        "\n",
        "    return entries"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZd6Ewm7xWw9"
      },
      "source": [
        "# Pull the data, and show one of the examples\n",
        "example_ticker = 'AMZN'\n",
        "sec_data = {}\n",
        "\n",
        "for ticker, cik in cik_lookup.items():\n",
        "    sec_data[ticker] = get_sec_data(cik, '10-K')\n",
        "\n",
        "# pprint.pprint(sec_data[example_ticker][:5])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upsI__AP0FFW",
        "outputId": "26b0a6fd-7b8e-4b1f-e328-69e08acb96bb"
      },
      "source": [
        "print(len(sec_data))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAyoD2Kx0bWw",
        "outputId": "1be0d464-9652-4ccf-9739-8973599e1520"
      },
      "source": [
        "# Download fillings from the urls we get in last step\n",
        "raw_fillings_by_ticker = {}\n",
        "\n",
        "for ticker, data in sec_data.items():\n",
        "    raw_fillings_by_ticker[ticker] = {}\n",
        "    for index_url, file_type, file_date in tqdm(data, desc='Downloading {} Fillings'.format(ticker), unit='filling'):\n",
        "        if (file_type == '10-K'):\n",
        "            file_url = index_url.replace('-index.htm', '.txt').replace('.txtl', '.txt')            \n",
        "            \n",
        "            raw_fillings_by_ticker[ticker][file_date] = sec_api.get(file_url)\n",
        "\n",
        "\n",
        "# print('Example Document:\\n\\n{}...'.format(next(iter(raw_fillings_by_ticker[example_ticker].values()))[:1000]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading CBG Fillings: 100%|██████████| 21/21 [00:16<00:00,  1.26filling/s]\n",
            "Downloading CBK Fillings: 100%|██████████| 29/29 [00:07<00:00,  3.72filling/s]\n",
            "Downloading CBLI Fillings: 100%|██████████| 15/15 [00:06<00:00,  2.15filling/s]\n",
            "Downloading CBM Fillings: 0filling [00:00, ?filling/s]\n",
            "Downloading CBMG Fillings: 100%|██████████| 18/18 [00:09<00:00,  1.82filling/s]\n",
            "Downloading CBMX Fillings: 100%|██████████| 11/11 [00:04<00:00,  2.64filling/s]\n",
            "Downloading CBPO Fillings: 100%|██████████| 20/20 [00:04<00:00,  4.37filling/s]\n",
            "Downloading CBS Fillings: 100%|██████████| 29/29 [00:08<00:00,  3.56filling/s]\n",
            "Downloading CBSH Fillings: 100%|██████████| 28/28 [00:12<00:00,  2.31filling/s]\n",
            "Downloading CBT Fillings: 100%|██████████| 27/27 [00:08<00:00,  3.14filling/s]\n",
            "Downloading CBZ Fillings: 100%|██████████| 6/6 [00:00<00:00,  7.45filling/s]\n",
            "Downloading CCBG Fillings: 100%|██████████| 32/32 [00:12<00:00,  2.55filling/s]\n",
            "Downloading CCC Fillings: 100%|██████████| 27/27 [00:05<00:00,  4.82filling/s]\n",
            "Downloading CCCR Fillings: 100%|██████████| 7/7 [00:04<00:00,  1.42filling/s]\n",
            "Downloading CCE Fillings: 100%|██████████| 7/7 [00:06<00:00,  1.00filling/s]\n",
            "Downloading CCF Fillings: 100%|██████████| 31/31 [00:14<00:00,  2.21filling/s]\n",
            "Downloading CCI Fillings: 100%|██████████| 27/27 [00:13<00:00,  2.01filling/s]\n",
            "Downloading CCK Fillings: 100%|██████████| 21/21 [00:16<00:00,  1.30filling/s]\n",
            "Downloading CCL Fillings: 100%|██████████| 32/32 [00:06<00:00,  5.00filling/s]\n",
            "Downloading CCMP Fillings: 100%|██████████| 22/22 [00:11<00:00,  2.00filling/s]\n",
            "Downloading CCNE Fillings: 100%|██████████| 27/27 [00:13<00:00,  1.99filling/s]\n",
            "Downloading CCO Fillings: 100%|██████████| 17/17 [00:10<00:00,  1.60filling/s]\n",
            "Downloading CCOI Fillings: 100%|██████████| 19/19 [00:08<00:00,  2.21filling/s]\n",
            "Downloading CCRN Fillings: 100%|██████████| 22/22 [00:15<00:00,  1.40filling/s]\n",
            "Downloading CCU Fillings: 100%|██████████| 28/28 [00:10<00:00,  2.63filling/s]\n",
            "Downloading CCUR Fillings: 100%|██████████| 30/30 [00:09<00:00,  3.23filling/s]\n",
            "Downloading CCXI Fillings: 100%|██████████| 9/9 [00:05<00:00,  1.75filling/s]\n",
            "Downloading CDNS Fillings: 100%|██████████| 31/31 [00:05<00:00,  5.35filling/s]\n",
            "Downloading CDR Fillings: 100%|██████████| 27/27 [00:11<00:00,  2.27filling/s]\n",
            "Downloading CDTI Fillings:  41%|████▏     | 12/29 [00:05<00:05,  2.92filling/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y74gzwuRxWzg"
      },
      "source": [
        "# Get documents from the fillings\n",
        "import re\n",
        "\n",
        "# To return a list of documents from a filling\n",
        "def get_documents(text):\n",
        "    extracted_docs = []\n",
        "    \n",
        "    doc_start_pattern = re.compile(r'<DOCUMENT>')\n",
        "    doc_end_pattern = re.compile(r'</DOCUMENT>')   \n",
        "    \n",
        "    doc_start_is = [x.end() for x in doc_start_pattern.finditer(text)]\n",
        "    doc_end_is = [x.start() for x in doc_end_pattern.finditer(text)]\n",
        "    \n",
        "    for doc_start_i, doc_end_i in zip(doc_start_is, doc_end_is):\n",
        "            extracted_docs.append(text[doc_start_i:doc_end_i])\n",
        "    \n",
        "    return extracted_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qfrtoi41xW10"
      },
      "source": [
        "# Extract the documents for the company\n",
        "filling_documents_by_ticker = {}\n",
        "\n",
        "for ticker, raw_fillings in raw_fillings_by_ticker.items():\n",
        "    filling_documents_by_ticker[ticker] = {}\n",
        "    for file_date, filling in tqdm(raw_fillings.items(), desc='Getting Documents from {} Fillings'.format(ticker), unit='filling'):\n",
        "        filling_documents_by_ticker[ticker][file_date] = get_documents(filling)\n",
        "\n",
        "\n",
        "#print('\\n\\n'.join([\n",
        "#    'Document {} Filed on {}:\\n{}...'.format(doc_i, file_date, doc[:200])\n",
        "#    for file_date, docs in filling_documents_by_ticker[example_ticker].items()\n",
        "#    for doc_i, doc in enumerate(docs)][:3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I57zZ3gPxW4C"
      },
      "source": [
        "# Define the function to get documents according to the type\n",
        "def get_document_type(doc):\n",
        "    type_pattern = re.compile(r'<TYPE>[^\\n]+')\n",
        "    \n",
        "    doc_type = type_pattern.findall(doc)[0][len('<TYPE>'):] \n",
        "    \n",
        "    return doc_type.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpL0ILncxW6I"
      },
      "source": [
        "# Only get the 10-k documents for the companies we selected\n",
        "ten_ks_by_ticker = {}\n",
        "\n",
        "for ticker, filling_documents in filling_documents_by_ticker.items():\n",
        "    ten_ks_by_ticker[ticker] = []\n",
        "    for file_date, documents in filling_documents.items():\n",
        "        for document in documents:\n",
        "            if get_document_type(document) == '10-k':\n",
        "                ten_ks_by_ticker[ticker].append({\n",
        "                    'cik': cik_lookup[ticker],\n",
        "                    'file': document,\n",
        "                    'file_date': file_date})\n",
        "\n",
        "\n",
        "# project_helper.print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['cik', 'file', 'file_date'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAfuzMzxy3hU"
      },
      "source": [
        "# Preprocess the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plHRrqR-y-7Q"
      },
      "source": [
        "### Clean up - remove the html tags and lowercase all the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwtK3X5txrSm"
      },
      "source": [
        "from w3lib.html import remove_tags\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    #text = BeautifulSoup(text, 'html.parser').get_text()\n",
        "    text = remove_tags(text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = remove_html_tags(text)\n",
        "    \n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOveLpVwxrVi"
      },
      "source": [
        "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
        "    for ten_k in tqdm(ten_ks, desc='Cleaning {} 10-Ks'.format(ticker), unit='10-K'):\n",
        "      #if ten_k['file']:\n",
        "        ten_k['file_clean'] = clean_text(ten_k['file'])\n",
        "\n",
        "# project_helper.print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['file_clean'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHM8timSy-1H"
      },
      "source": [
        "### Lemmatize\n",
        "Grouping together various inflections of a word to analyze them as a single item, identified by the word’s lemma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teaWiWaFxrZl"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "\n",
        "def lemmatize_words(words):\n",
        "    lemmatized_words = [WordNetLemmatizer().lemmatize(word, 'v') for word in words]\n",
        "    return lemmatized_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWsqCScNxrb6"
      },
      "source": [
        "word_pattern = re.compile('\\w+')\n",
        "\n",
        "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
        "    for ten_k in tqdm(ten_ks, desc='Lemmatize {} 10-Ks'.format(ticker), unit='10-K'):\n",
        "        ten_k['file_lemma'] = lemmatize_words(word_pattern.findall(ten_k['file_clean']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msTvnpqOzTTb"
      },
      "source": [
        "### Remove Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_y_W1lxxrev"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "lemma_english_stopwords = lemmatize_words(stopwords.words('english'))\n",
        "\n",
        "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
        "    for ten_k in tqdm(ten_ks, desc='Remove Stop Words for {} 10-Ks'.format(ticker), unit='10-K'):\n",
        "        ten_k['file_lemma'] = [word for word in ten_k['file_lemma'] if word not in lemma_english_stopwords]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlQngtngzspT"
      },
      "source": [
        "Here, the keys for each ten_k is ['cik', 'file', 'file_date', 'file_clean', 'file_lemma']."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHp4Fg8Tzn8-"
      },
      "source": [
        "### Transform the Data Format - from Dict to DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7mucCvpxrjz"
      },
      "source": [
        "ten_ks_df_dict = {'date': [], 'company': [], 'ticker': [], 'doc': []}\n",
        "\n",
        "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
        "    for ten_k in ten_ks:\n",
        "        ten_ks_df_dict['date'].append(ten_k['file_date'])\n",
        "        ten_ks_df_dict['company'].append(ticker)\n",
        "        ten_ks_df_dict['ticker'].append(cik_lookup[ticker])\n",
        "        #ten_ks_df_dict['lemma'].append(ten_k['file_lemma'])\n",
        "        ten_ks_df_dict['doc'].append(' '.join(ten_k['file_lemma']))\n",
        "\n",
        "ten_ks_df = pd.DataFrame(ten_ks_df_dict)\n",
        "\n",
        "ten_ks_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WzfNmhbxrhp"
      },
      "source": [
        "ten_ks_df.to_csv('1.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}